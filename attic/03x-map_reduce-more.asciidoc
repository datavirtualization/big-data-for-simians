=== Santa's Row-Path Delivery System ===

The new system is great and toys are piling up at a prodigious rate when one of the elves points out a problem: the toys are piling up at a prodigious rate. They're in piles by type of toy (dolls, robots, ponies) when they need to be organized for delivery by Santa (23 Evergreen Terrace, 24 Evergreen Terrace, ...). The new toy production pipeline has to be integrated with the toy delivery operation.
/// It would help readers to have a tie in here, analogy to real world. "...this is similar to a business problem in which a security manager might want to..." Amy////

==== Santa's High-Throughput Delivery Pipeline

As much consternation as Chimpanzee and Elephant felt viewing the Elves' original process flow, it was pure professional admiration they felt on visiting the elves' package staging warehouse. "This, friend, is how you build for throughput!", exclaimed JT. "Indeed", said Nanette, "and look how directly the layout of the gift sacks is aligned with the access pattern in production".

As Nanette said, this process is defined by how its outputs are used, not by how they're produced, so a brief word about the delivery system. (We'll also need to know about this for a coding project with the reindeer coming shortly). Starting at sundown of the 25th on Tomorrow Island footnote:[No, really: there's a part of Alaska split by the date line, one half called Tomorrow Island and the other called Yesterday Island] just west of the Date Line, Santa Claus races against the sun to deliver the last presents to good little boys and girls before sunup on Yesterday Island, just east of it. From Tomorrow Island, the reindeer race south along the Date Line towards New Zealand; once at their southern extent, they loop north again along the 179&deg; meridian, and so on, covering the entire globe in tall scan-line columns.

In this manner, Santa is able to cover a large three-dimensional extent (longitude, latitude and chimney/elevator shaft) with only one-dimensional traversal. Take a moment to consider how cool that is -- suppose Santa had a webcam in his sleigh taking selfies every five minutes the whole trip (or any other data ordered and indexed by the time of day). To see all the photos within a given region of the globe, you only have to scan through a few continuous ranges of indexed photos. Since "process a whole ton of records all in a row" is the thing our one-trick elephant does best, this is a key pattern for giving Hadoop mastery over complex spaces.

The gifts are stored to match this pattern of access.
(...)
The gifts for a household are stored together in a sack. The sacks are stored together in shipping containers, in order front-to-back of Santa's visit time. All Mr. Claus has to do is pull the next sack off the front of the current container and dive down the next chimney.

.An ASCII-art
----
        _
    +m-o-O-x-q-+     +m-o-O-x-q-+     +m-o-O-x-q-+     +m-o-O-x-q-+              +m-o-O-x-q-+
    | Tomorrow | ... | New      | ... | Austr    | ... | Japan    | ... ... ...  | Yest'day |
    | Island   |     | Zealand  |     | alia     |     |          | 	         | Island   |
    +----------+     +----------+     +----------+     +----------+ 	         +----------+
----


// footnote:[My mom invented the Hannukka Train for my brothers and I as the equivalent of a
// Christmas Tree: gift-wrapped "box"cars, with paper plate wheels, to hold the gifts. It's an idea
// worth popularizing.]

==== Partition, Sort and Grouping of Gifts

(TODO: organize the next few paragraphs)

Santa's sacks are packed by the boisterous members of the Elven Stevedore's Union

Santa's route was divided into dozens of segments, each with its own shipping clerk.
Every neighborhood Santa visits
is encoded by when along his journey Santa will visit that household: all the 20130's come before all the 20131's, etc. (By clever construction, this figure using only the house's location)

The toys for each family need to be placed together in sacks, and those sacks placed together based on when they'll be delivered.

There's one shipping container for each 1&deg; longitude/1&deg; latitude cell,
but not all containers get the same number of gifts.
Each stevedore oversees an arbitrary collection of containers:
It's necessary that the gift sacks loaded into each container are strictly sorted by delivery order,
but it's not important that the containers be held in any order until launch day.

----
   Drawing of a tattoo-ed Teamster-looking elf with
   a clipboard and half-frame old man glasses
----   

/// Good spot to insert a discussion of a real world comparable situation.  Amy////

==== Chimpanzee and Elephant mail some toys

As you might guess: out come the chimps with typewriters, in march the elephants (with cargo racks in place of file folders), up spring a squadron of elven shipping clerks, and off go the presents through the following three-step flow.

In the first phase, chimpanzees receive the presents one-by-one as they are produced, and attach a three-part label based on the destination household.
The first part of the label (the "partition" key) specifies the 1&deg; by 1&deg; geographic cell -- and thus the shipping container -- for the address.
The second part of the label (the "sort" key) gives the visit-order of that household within the cell (derivable by the longitude/latitude)
The third part of the label (the "group" key) is the name of the family.


Elephants deliver toys to the stevedore
in charge of that shipping container.
A stevedore supervises many shipping containers;
  this evens out the load, or otherwise you'd overwork the Frankfurt-Zurich-Tunis shipping clerk to death while the Atlantic Ocean-only ones did the crossword puzzle.


The gifts are sorted on the back of each bull elephant
letting the stevedores use the same "merge sort" technique:
simply pluck the nextmost gift from among the waiting elephants.

=== Close Encounters of the Reindeer Kind (pt 1)

Let's process some real data and, with Santa's modernized gift-handling system in mind, see how Hadoop orchestrates the flow of data from mappers to reducers. 

While Santa is busy year-round, his Reindeer spend their multi-month break between holiday seasons pursuing their favorite hobby: UFOlogy (the study of Unidentified Flying Objects and the search for extraterrestrial civilization). So you can imagine how excited they were to learn about the http://www.infochimps.com/datasets/60000-documented-ufo-sightings-with-text-descriptions-and-metada[National UFO Reporting Center] data set: 60,000 documented UFO sightings for a sharp-nosed reindeer to investigate. They'd like to send a team to investigate each sighting based on the category of spacecraft: one team investigates multiple-craft formations, one investigates fireballs, and so on. So the first thing to do is assign each sighting to the right team. Since sixty thousand sightings is much higher than a reindeer can count (only four hooves!), let's help them organize the data. (Sixty thousand records is much too small for Hadoop to be justified, but it's the perfect size to learn with.)
//// Tie in.  "...so, similarly, if you were Toyota, and you wanted to record signtings of electric cars (or electric car charging stations in the most populated cities in the United States..."  Amy////

=== Locality of Reference ===

//// Add something along the lines of, "You'll recall our discussion about locality.  Remember how we learned that x, y, and z have...?  Now you'll be able to see that in action."  Amy////

Ever since there were 

        [] [] [] [] 
        [] [] [] [] eight computers and a network,
    
programmers have wished that

       eight computers   [] [] [] [] [] [] [] [] []
       solved problems
       twice as fast as
       four computers    [] [] [] []

The problem comes

          when the computer over here \/ 
                          [] [] [] [] [] [] []    needs data from  
                          [] [] [] [] [] [] [] <- the one over here
                          [] [] [] [] []<+]-+]--- or the one here
	  and this one -> [] [] [] [] [] [] []
             needs data from this one ^^
    and so forth. 

In the example of the Elves, the data was at points organized in three different ways:

1. In mailbags corresponding to the post office and date each letter was mailed
2. In pygmy-elephant batches according to toy type, sorted by toy detail.
3. In sacks of gifts for each household, ordered by Santa's delivery path.

The difficulty the elves faced wasn't in processing the records. Workforms and Gift Making, that's what they were born to do. Their problem was first, that toy requests were organized by mail route, not toy type; and next, that the gifts for each household were organized by toy type, not geography. In both cases, the job of the chimps was to transform the data and then label it by its _key locality_: in these examples, toy type and toy detail, or delivery path and household.   //// This seems unclear. Are you saying that toy type and toy deail ARE the key localities?  This is an important point to clearly define. Amy////

//// Correlate the above to equivalent real-world scenarios and explain the "key localities" of those examples too.  Amy////

=== Locality: Examples ===

This book is fundamentally about just that thing: helping you identify the key locality of a data problem. /// Something like, "...in other words, the common center of a data problem.  Or, locality could refer to the universals in a set, such as blood type or eye color in the case of..."  (I don't know if that is correct but I hope you get what I'm suggesting with these examples.)  Amy//// Now for folks who learn by reading, what we mean by "key locality" is starting to make sense, and so I'll give you some brief examples. For folks who learn by doing, you might want to read some more examples and then 

* *word count*: You can count the occurrence of every word in a large body of text by a) grouping all ocurrences of each word together b) counting each group. In this case, the key locality is the word itself. // the words are initially organized by position within the text; 

* *total sort*: To sort a large number of names, group each name by its first few letters (eg "Aaron" = "aa", "Zoe" = "zo"), making each group small enough to efficiently sort in memory. Reading the output of each group in the order of its label will produce the whole dataset in order. The key locality is to partition by prefix and sort by name.

* *network graph statistics*: Counting the average number of Twitter messages per day in each user's timeline requires two steps. First, take every 'A follows B' link and get it into the same locality as the user record for its "B".  Next, group all those links by the user record for 'A', and sum their messages-per-day. 

* *correlation*: Correlate stock prices with pageview counts of each corporation's Wikipedia pages: bring the stock prices and pageview counts for each stock symbol together, and sort them together by time. 

//// I'm wondering if it would be worthwhile for readers to get the best possible hang of locality if you were to guide them through actually coding the above four? That would be helpful for the folks who learn by doing, as you refer to above. Amy////

=== The Hadoop Haiku ===

You can try to make it efficient for any computer to talk to any other computer. But it requires top-of-the-line  hardware, and clever engineers to build it, and a high priesthood to maintain it, and this attracts project managers, which means meetings, and soon everything is quite expensive, so expensive that only nation states and huge institutions can afford to do so. This of course means you can only use these expensive supercomputer for Big Important Problems -- so unless you take drastic actions, like joining the NSA or going to grad school, you can't get to play on them.

Instead of being clever, be simple.

Map/Reduce proposes this fair bargain. You must agree to write only one type of program, one that's as simple and constrained as a haiku. 

.The Map/Reduce Haiku
----
      data flutters by
          elephants make sturdy piles
        insight shuffles forth
----

If you do so, Hadoop will provide near-linear scaling on massive numbers of machines, a framework that hides and optimizes the complexity of distributed computing, a rich ecosystem of tools, and one of the most adorable open-source project mascots to date.

More prosaically, 

1. *label*      -- turn each input record into any number of labelled records
2. *group/sort* -- hadoop groups those records uniquely under each label, in a sorted order
3. *reduce*     -- for each group, process its records in order; emit anything you want.

The trick lies in the 'group/sort' step: assigning the same label to two records in the 'label' step ensures that they will become local in the reduce step.

Let's join back up with the Chimpanzee and Elephant Shipping Company and see this in practice.
