









mapred.tasktracker.map.tasks.maximum
mr










mapred.tasktracker.map.tasks.maximum






mapred.tasktracker.reduce.tasks.maximum
mr










mapred.tasktracker.reduce.tasks.maximum


1
mem-m
dfs.block.size
dfs
67108864
134217728
134217728




dfs.block.size
The default block size for new files. Apache default is 67108864 (64MB); CDH3 is 128MB
2
mem-m
fs.s3.block.size
cor
67108864


134217728




fs.s3.block.size
Block size to use when writing files to S3.
3
mem-m
fs.s3n.block.size
cor
67108864


134217728




fs.s3n.block.size
Block size to use when reading files using the native S3 filesystem (s3n: URIs).
18
mem-m
mapred.min.split.size
mr
0








mapred.min.split.size
The minimum size chunk that map input should be split into. Note that some file formats may have minimum split sizes that take priority over this setting.
4
mem-m
io.sort.mb
mr
100


226
225
block size * 1.2 safety factor / ( (1-i.s.record.percent) * i.s.spill.percent)
io.sort.mb
The cumulative size of the serialization and accounting buffers storing records emitted from the map, in megabytes.
5
mem-m
io.sort.record.percent
mr
0.05




0.15


io.sort.record.percent
The ratio of serialization to accounting space can be adjusted. Each serialized record requires 16 bytes of accounting information in addition to its serialized size to effect the sort. This percentage of space allocated from io.sort.mb affects the probability of a spill to disk being caused by either exhaustion of the serialization buffer or the accounting space. Clearly, for a map outputting small records, a higher value than the default will likely decrease the number of spills to disk.
6
mem-m
io.sort.spill.percent
mr
0.8




0.8
16 / (16 + rec.size in bytes); check the "Map Output Records" vs "Map Output Bytes" counters
io.sort.spill.percent
This is the threshold for the accounting and serialization buffers. When this percentage of either buffer has filled, their contents will be spilled to disk in the background. Let io.sort.record.percent be r, io.sort.mb be x, and this value be q. The maximum number of records collected before the collection thread will spill is r * x * q * 2^16. Note that a higher value may decrease the number of- or even eliminate- merges, but will also increase the probability of the map task getting blocked. The lowest average map times are usually obtained by accurately estimating the size of the map output and preventing multiple spills.
7


max mb per spill










153
max mb per spill


8


block fraction overage










120%
block fraction overage


9


max records per spill










1,769,472
max records per spill


10


"small" record breakeven, bytes










91
"small" record breakeven, bytes


13
mem-m
io.file.buffer.size


4096
65536
65536




io.file.buffer.size
The size of buffer for use in sequence files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.
59
mem-m
dfs.replication
dfs
3








dfs.replication
Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time.
61
mem-m
mapred.child.java.opts
mr










mapred.child.java.opts
Java opts for the task tracker child processes. The following symbol, if present, will be interpolated: @taskid@ is replaced by current TaskID. Any other occurrences of '@' will go unchanged. For example, to enable verbose gc logging to a file named for the taskid in /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of: -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc The configuration variable mapred.child.ulimit can be used to control the maximum virtual memory of the child processes.
61
mem-m
mapred.map.child.java.opts
mr










mapred.map.child.java.opts


61
mem-r
mapred.reduce.child.java.opts
mr










mapred.reduce.child.java.opts


12




















6
mem-r
io.sort.factor
mr
10






covaries with io.sort.mb, io.file.buffer.size and your disk's latency/bandwidth characteristics
io.sort.factor
Specifies the number of segments on disk to be merged at the same time. It limits the number of open files and compression codecs during the merge. If the number of files exceeds this limit, the merge will proceed in several passes. Though this limit also applies to the map, most jobs should be configured so that hitting this limit is unlikely there.
7
mem-r
mapred.inmem.merge.threshold
mr
1000


0




mapred.inmem.merge.threshold
The number of sorted map outputs fetched into memory before being merged to disk. Like the spill thresholds in the preceding note, this is not defining a unit of partition, but a trigger. In practice, this is usually set very high (1000) or disabled (0), since merging in-memory segments is often less expensive than merging from disk (see notes following this table). This threshold influences only the frequency of in-memory merges during the shuffle. Note: When running with a combiner, the reasoning about high merge thresholds and large buffers may not hold. For merges started before all map outputs have been fetched, the combiner is run while spilling to disk. In some cases, one can obtain better reduce times by spending resources combining map outputs- making disk spills small and parallelizing spilling and fetching- rather than aggressively increasing buffer sizes. Also, when merging in-memory map outputs to disk to begin the reduce, if an intermediate merge is necessary because there are segments to spill and at least io.sort.factor segments already on disk, the in-memory map outputs will be part of the intermediate merge. From the XML: The threshold, in terms of the number of files for the in-memory merge process. When we accumulate threshold number of files we initiate the in-memory merge and spill to disk. A value of 0 or less than 0 indicates we want to DON'T have any threshold and instead depend only on the ramfs's memory consumption to trigger the merge
9
mem-r
mapred.job.shuffle.input.buffer.percent
mr
0.7






reducer heap * m.j.shuffle.in.buf.pct > typical map task out size
mapred.job.shuffle.input.buffer.percent
The percentage of memory- relative to the maximum heapsize as typically specified in mapred.child.java.opts- that can be allocated to storing map outputs during the shuffle. Though some memory should be set aside for the framework, in general it is advantageous to set this high enough to store large and numerous map outputs. The percentage of memory to be allocated from the maximum heap size to storing map outputs during the shuffle.
8
mem-r
mapred.job.shuffle.merge.percent
mr
0.66






if each reducer's total load is less than (reducer heap * m.j.shuffle.in.buf.pct * m.j.shuffle.merge.percent) they will never hit the disk
mapred.job.shuffle.merge.percent
The memory threshold for fetched map outputs before an in-memory merge is started, expressed as a percentage of memory allocated to storing map outputs in memory. Since map outputs that can't fit in memory can be stalled, setting this high may decrease parallelism between the fetch and merge. Conversely, values as high as 1.0 have been effective for reduces whose input can fit entirely in memory. This parameter influences only the frequency of in-memory merges during the shuffle. XML: The usage threshold at which an in-memory merge will be initiated, expressed as a percentage of the total memory allocated to storing in-memory map outputs, as defined by mapred.job.shuffle.input.buffer.percent
10
mem-r
mapred.job.reduce.input.buffer.percent
mr
0


0


if your reduce task itself doesn't need ram (eg for wukong jobs), set this to more like 0.7
mapred.job.reduce.input.buffer.percent
The percentage of memory relative to the maximum heapsize in which map outputs may be retained during the reduce. When the reduce begins, map outputs will be merged to disk until those that remain are under the resource limit this defines. By default, all map outputs are merged to disk before the reduce begins to maximize the memory available to the reduce. For less memory-intensive reduces, this should be increased to avoid trips to disk. XML: The percentage of memory- relative to the maximum heap size- to retain map outputs during the reduce. When the shuffle is concluded, any remaining map outputs in memory must consume less than this threshold before the reduce can begin.
11
mem-r
mapred.reduce.parallel.copies
mr
5
10




reducers * m.r.parallel.copies ~ machines * tt.http.threads; also affected by m.r.slowstart.completed.maps
mapred.reduce.parallel.copies
The default number of parallel transfers run by reduce during the copy(shuffle) phase.
12




















24
mem
datanode heap size
mr




384




datanode heap size


25
mem
tasktracker heap size
mr




384




tasktracker heap size


35




















36
disk
mapred.output.compress
mr
FALSE


TRUE




mapred.output.compress
Should the job outputs be compressed?
39
disk
mapred.compress.map.output
mr
FALSE


FALSE




mapred.compress.map.output
Should the outputs of the maps be compressed before being sent across the network. Uses SequenceFile compression.
37
disk
mapred.output.compression.type
mr
RECORD


SnappyCodec




mapred.output.compression.type
If the job outputs are to compressed as SequenceFiles, how should they be compressed? Should be one of NONE, RECORD or BLOCK.
38
disk
mapred.output.compression.codec
mr
DefaultCodec


SnappyCodec




mapred.output.compression.codec
If the job outputs are compressed, how should they be compressed?
40
disk
mapred.map.output.compression.codec
mr
DefaultCodec








mapred.map.output.compression.codec
If the map outputs are compressed, how should they be compressed?
41
disk
io.seqfile.compress.blocksize
cor
1000000








io.seqfile.compress.blocksize
The minimum block size for compression in block compressed SequenceFiles.
42
disk
io.compression.codecs
cor




add Snappy




io.compression.codecs


21




















45
exec
mapred.reduce.slowstart.completed.maps
mr
0.05








mapred.reduce.slowstart.completed.maps
Fraction of the number of maps in the job which should be complete before reduces are scheduled for the job.
15
exec
mapred.map.tasks.speculative.execution
mr
TRUE
TRUE
TRUE




mapred.map.tasks.speculative.execution
If true, then multiple instances of some map tasks may be executed in parallel. Set this to false on any map-only job that will write to a database or other external resource.
16
exec
mapred.reduce.tasks.speculative.execution
mr
TRUE
FALSE
FALSE




mapred.reduce.tasks.speculative.execution
If true, then multiple instances of some reduce tasks may be executed in parallel.
58




















22




















23


Hadoop












Hadoop


26
exec
mapred.job.reuse.jvm.num.tasks
mr
1


-1




mapred.job.reuse.jvm.num.tasks


27
exec
mapred.child.ulimit
mr










mapred.child.ulimit
The maximum virtual memory, in KB, of a process launched by the Map-Reduce framework. This can be used to control both the Mapper/Reducer tasks and applications using Hadoop Pipes, Hadoop Streaming etc. By default it is left unspecified to let cluster admins control it via limits.conf and other such relevant mechanisms. Note: mapred.child.ulimit must be greater than or equal to the -Xmx passed to JavaVM, else the VM might not start




mapred.map.child.ulimit












mapred.map.child.ulimit






mapred.reduce.child.ulimit












mapred.reduce.child.ulimit




exec
mapred.jobtracker.restart.recover
mr
FALSE




TRUE


mapred.jobtracker.restart.recover
"true" to enable (job) recovery upon restart, "false" to start afresh






















29


dfs.datanode.max.xcievers












dfs.datanode.max.xcievers


30
net
mapred.job.tracker.handler.count
mr
10


40
64


mapred.job.tracker.handler.count
The number of server threads for the JobTracker. This should be roughly 4% of the number of tasktracker nodes.
33
net
dfs.namenode.handler.count
dfs
10


40
64


dfs.namenode.handler.count
The number of server threads for the namenode.
31
net
tasktracker.http.threads
mr
40
46
32




tasktracker.http.threads
The number of worker threads that for the http server. This is used for map output fetching
32
net
dfs.datanode.handler.count
dfs
3


8




dfs.datanode.handler.count
The number of server threads for the datanode.
62
exec
fs.s3.maxRetries
cor
4








fs.s3.maxRetries
The maximum number of retries for reading or writing files to S3, before we signal failure to the application.
63
exec
fs.s3.sleepTimeSeconds
cor
10








fs.s3.sleepTimeSeconds
The number of seconds to sleep between each S3 retry.
49




















50
usage
mapred.heartbeats.in.second
mr
100








mapred.heartbeats.in.second
Expert: Approximate number of heart-beats that could arrive at JobTracker in a second. Assuming each RPC can be processed in 10msec, the default value is made 100 RPCs in a second.


usage
mapreduce.tasktracker.outofband.heartbeat
mr
FALSE




TRUE


mapreduce.tasktracker.outofband.heartbeat
Expert: Set this to true to let the tasktracker send an out-of-band heartbeat on task-completion for better latency.
51
usage
mapred.disk.healthChecker.interval
mr
60000








mapred.disk.healthChecker.interval
How often the TaskTracker checks the health of its local directories. Configuring this to a value smaller than the heartbeat interval is equivalent to setting this to heartbeat interval value.
52
usage
mapred.healthChecker.interval
mr
60000








mapred.healthChecker.interval
Frequency of the node health script to be run, in milliseconds. Default 1 minute
53
usage
dfs.df.interval
dfs
60000


300000




dfs.df.interval
Disk usage statistics refresh interval in msec.
54
usage
dfs.blockreport.intervalMsec
dfs
3600000








dfs.blockreport.intervalMsec
Determines block reporting interval in milliseconds.
55
usage
dfs.heartbeat.interval
dfs
3








dfs.heartbeat.interval
Determines datanode heartbeat interval in seconds.
56
usage
fs.checkpoint.period
cor
3600








fs.checkpoint.period
The number of seconds between two periodic checkpoints.
57
usage
fs.checkpoint.size
cor
67108864








fs.checkpoint.size
The size of the current edit log (in bytes) that triggers a periodic checkpoint even if the fs.checkpoint.period hasn't expired.
44




















28
usage
mapred.submit.replication
mr
10








mapred.submit.replication
The replication level for submitted job files. This should be around the square root of the number of nodes.
17




















19
exec
mapred.max.tracker.blacklists
mr
4








mapred.max.tracker.blacklists
The number of blacklists for a taskTracker by various jobs after which the task tracker could be blacklisted across all jobs. The tracker will be given a tasks later (after a day). The tracker will become a healthy tracker after a restart
20
exec
mapred.max.tracker.failures
mr
4








mapred.max.tracker.failures
The number of task-failures on a tasktracker of a given job after which new tasks of that job aren't assigned to it.
47




















48
disk
dfs.datanode.du.reserved
dfs
0
1073741824






dfs.datanode.du.reserved
Reserved space in bytes per volume. Always leave this much space free for non dfs use.
64




















65


Logging












Logging
http://www.cloudera.com/blog/2010/11/hadoop-log-location-and-retention/
66
interact
fs.trash.interval
cor
0


4320




fs.trash.interval
Number of minutes between trash checkpoints. If zero, the trash feature is disabled.
67
interact
mapreduce.job.counters.max
mr
120








mapreduce.job.counters.max


68
interact
mapreduce.job.counters.groups.max
mr
50








mapreduce.job.counters.groups.max


69
interact
webinterface.private.actions
cor
FALSE


FALSE
TRUE


webinterface.private.actions
If set to true, the web interfaces of JT and NN may contain actions, such as kill job, delete file, etc., that should not be exposed to public. Enable this option if the interfaces are only reachable by those who have the right authorization.
46
log
mapred.combine.recordsBeforeProgress
mr
10000








mapred.combine.recordsBeforeProgress
The number of records to process during merge before sending a progress notification to the TaskTracker.
73
log
mapred.job.tracker.persist.jobstatus.active
mr
FALSE




TRUE


mapred.job.tracker.persist.jobstatus.active
Indicates if persistency of job status information is active or not.
70
log
mapred.jobtracker.completeuserjobs.maximum
mr
100




1000


mapred.jobtracker.completeuserjobs.maximum
The maximum number of complete jobs per user to keep around before delegating them to the job history.
71
log
mapred.userlog.limit.kb
mr
0




10100100


mapred.userlog.limit.kb
The maximum allowed size of the user jobconf.
72
log
mapred.userlog.retain.hours
mr
24




240


mapred.userlog.retain.hours
The maximum time, in hours, for which the user-logs are to be retained after the job completion.
74
log
mapred.job.tracker.persist.jobstatus.hours
mr
0




240


mapred.job.tracker.persist.jobstatus.hours
The number of hours job status information is persisted in DFS. The job status information will be available after it drops of the memory queue and between jobtracker restarts. With a zero value the job status information is not persisted at all in DFS.
75
log
mapred.jobtracker.retirejob.check
mr






1200000


mapred.jobtracker.retirejob.check




log
mapred.jobtracker.retirejob.interval
mr






864000000


mapred.jobtracker.retirejob.interval
























76
log-prof
mapred.task.profile
mr
FALSE








mapred.task.profile
To set whether the system should collect profiler information for some of the tasks in this job? The information is stored in the user log directory. The value is "true" if task profiling is enabled.
77
log-prof
mapred.task.profile.maps
mr
0-2








mapred.task.profile.maps
To set the ranges of map tasks to profile. mapred.task.profile has to be set to true for the value to be accounted.
78
log-prof
mapred.task.profile.reduces
mr
0-2








mapred.task.profile.reduces
To set the ranges of reduce tasks to profile. mapred.task.profile has to be set to true for the value to be accounted.
79




















80
resize
topology.node.switch.mapping.impl
cor










topology.node.switch.mapping.impl
The default implementation of the DNSToSwitchMapping. It invokes a script specified in topology.script.file.name to resolve node names. If the value for topology.script.file.name is not set, the default value of DEFAULT_RACK is returned for all node names.
81
resize
topology.script.file.name
cor










topology.script.file.name
The script name that should be invoked to resolve DNS names to NetworkTopology names. Example: the script would take host.foo.bar as an argument, and return /rack1 as the output.
81
resize
net.topology.table.file.name
cor


(patched)






net.topology.table.file.name
The file name for a topology file, which is used when the topology.script.file.name property is set to org.apache.hadoop.net.TableMapping. The file format is a two column text file, with columns separated by whitespace. The first column is a DNS or IP address and the second column specifies the rack where the address maps. If no entry corresponding to a host in the cluster is found, then /default-rack is assumed.
82
resize
dfs.balance.bandwidthPerSec
dfs


1048576
1100100
20100100


dfs.balance.bandwidthPerSec
Specifies the maximum amount of bandwidth that each datanode can utilize for the balancing purpose in term of the number of bytes per second.
83
resize
dfs.namenode.decommission.nodes.per.interval
dfs


5
20




dfs.namenode.decommission.nodes.per.interval



