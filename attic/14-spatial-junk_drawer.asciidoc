
 (for our purposes represented as bounding rectangles, arbitrary polygons and occasionally circles).


* Mechanics:
  - for point, path, polygon, polygon with hole in it:
      - Area, IsClosed, Dimensionality, Number of Coordinates, Number of Points
      - Centroid, Envelope
      - convex hull, simplify, densify, buffer, generalize, offset, transform
      - (polygon should have a hole; a convex hull different than itself and a circle; enough but not too much complexity)
      - exploding shape into points
  - for two polygons, and part is in the hole of a polygon
      - intersect, XOR, a-b, b-a, union)
      -

* Tiles (Sp Agg Pts)
* Decompose Tiles (Sp Agg Regions)
* Z-order Tiles (
* Naive Join Points (9 tiles)
* Point to bbox; sp join
* Spatial Merge Join
* Voronoi
* Weather for Stadium

* **Spatial Aggregation**
  - Smoothing Pointwise Data Locally (Spatial Aggregation of Points)
  - Creating a Spatial Density Map

* **Spatial Data**
  - Points, Paths and Regions
  - Geometry Primitives: Points, Polygons and so forth
  - Longitude and Latitude, Points and Features
  - Always: longitude then latitude (x then y); always: minx, miny, maxx, maxy.
  - GeoJSON, WKT, etc

* **Working with Regions**
  - Smoothing Regional Data onto a Consistent Grid (Spatial Aggregation of Regions)
    - another way to do this later under voronoi
  - **Geographic Projections**
    - Equirectangular, Web-Mercator, Lambert, CoolHat, HEALPix
    - important that it have a well-conditioned inverse
    - CRS, datum, spatial reference
    - concerns are not the same as for other uses
    - http://spatialreference.org/
    - http://www.sharpgis.net/post/2007/05/05/Spatial-references2c-coordinate-systems2c-projections2c-datums2c-ellipsoids-e28093-confusing
    - Datum: http://webhelp.esri.com/arcgisdesktop/9.2/index.cfm?TopicName=Datums
    - Spheroid: http://webhelp.esri.com/arcgisdesktop/9.2/index.cfm?TopicName=Datums
      - The spheroid parameters for the World Geodetic System of 1984 (WGS 1984 or WGS84) are: a = 6378137.0 meters; b = 6356752.31424 meters; 1/f = 298.257223563.
      
//    - Some of the more commonly used spatial reference systems are: 4326 - WGS 84 Long Lat, 4269 - NAD 83 Long Lat, 3395 - WGS 84 World Mercator, 2163 - US National Atlas Equal Area, Spatial reference systems for each NAD 83, WGS 84 UTM zone - UTM zones are one of the most ideal for measurement, but only cover 6-degree regions.


* **Join points**
    * points to tiles, cross points on tile, calculate distances
    * point to geographic radius shape, use contains
    * point to bounding box, index into quadtree, calculate distance

* Mechanics of Spatial Analysis
  -
    - ...
  - Spatial Relationship Tests

* **Grid Tiles and Quad Cells**
  - Exporting data for Presentation by a Tileserver
  - Key Strategic Pattern: Tile / Cull / Process

* Spatial Nearness Join (Within-X-Distance)
  - Matching Nearby Points (Pointwise Spatial Join)
  - Region Within a Geographic Distance of a Geometry
    - Shape; Bounding Box; cautions on spherical geometry

* Matching Points with the Regions


* Nearest-Feature Spatial Join
  - Voronoi Polygons turn Points into Regions
  - Turning Points of Measurements into Regions of Influence
  - Decompose Voronoi Polygons onto Grid Tiles
  - Map Observations to Grid Tiles
  - Joining Stadiums onto Grid Tiles
  - Finding Nearby Objects
  - Smoothing the Distribution
  - Results


* Mapping from earth to grid
* Extending the base of analytic patterns to two and more dimensions

long/lat implies elevation -- For the great majority of questions of interest, elevation is irrelevant, and so we discard it
We just need to rough-cut the data and then turn it over to dedicated spatial methods


Our approach will be consistently to
apply the toolkit of Analytic Patterns that we assembled in Chapters 4-9 (REF)
to put all relevant data into context,
and then turn to specialized geospatial libraries to
synthesize results

The large-scale part of this demands no great sophistication
We can pretend that circles are rectangular, that shapes do not ever have holes, that the earth is not only a perfect sphere but in fact is a planar grid. All manner of convenient distortions
so outrageous they literally tear the space-time continuum
are allowable as long as they obey the fundamental strategic rule:

* put all data that might form relevant context together

(TODO: better phrasing)


Our reindeer friends are deflated to learn that the two maps do not resemble each other.


==== Smoothing Regional Data onto a Consistent Grid (Spatial Aggregation of Regions)

...

===== Pattern in Use

* _Where You'll a Use It_:



=== Geographic Data Model ===

==== Geometry Primitives: Points, Polygons and so forth

Geographic data shows up in the form of

* Points -- a pair of coordinates. When given as an ordered pair (a "Position"), always use `[longitude,latitude]` in that order, matching the familiar `X,Y` order for mathematical points. When it's a point with other metadata, it's a Place footnote:[in other works you'll see the term Point of Interest ("POI") for a place.], and the coordinates are named fields.
* Paths -- an array of points `[[longitude,latitude],[longitude,latitude],...]`
* Region -- an array of paths, understood to connect and bound a region of space. `[ [[longitude,latitude],[longitude,latitude],...], [[longitude,latitude],[longitude,latitude],...]]`. Your array will be of length one unless there are holes or multiple segments
* "Bounding Box" (or `bbox`) -- a rectangular bounding region, `[-5.0, 30.0, 5.0, 40.0]`

Back in Chapter 4 (REF), we introduced the simple scalar types (numbers, strings, etc.) and three complex types (`tuple`, `bag`, and `map`). Since every spatial analysis exploration involves

One thing
Spatial analysis libraries
rely on the http://www.opengeospatial.org/[OGC (Open Geospatial Consortium)]

Geometry

Point, LineString, Polygon; and corresponding multi-part geometries MultiPoint, MultiLineString, MultiPolygon.

Behind these smiling friendly inviting abstraction
lies
a host of diabolical complexities

In regular usage, even double-precision floating-point math can introduce
discrepancies large enough to incalidate results
or present visual artifacts
-- pushing the boundary of a shape off the shape itself, causing tears or overlaps where there were none, turning small polygons into degenerate points, introduce numerical instability

But for the big data section of it, where we are chiefly concerned with relating data in context,
there are really only these

* a point in space
* a spatial extent -- paths, regions, etc
// * non-spatial data

In fact, we can go even farther:

* points
* rectangles

Remember, all we're trying to do is land all (possibly) related data onto the same reducer before we bring in the big guns.


=== Spatial Nearness Join (Within-X-Distance)

* quad cells have different sizes; lookup table

All sightings near airport

(dispatch to 9 nearest you)

==== Matching Nearby Points (Pointwise Spatial Join)

* Common sense tells you that a weather observation is generally valid for places within a few kilometers, but certainly not useful for places Hundreds of kilometers away. It would be useful to have a more precise guideline for the distance where a weather measurement should not be considered reliable.
    * first find all pairs of weather stations within 50 km of each other. Emit each pair of IDs along with their distance: put the lower-numbered ID in the first slot (making it easy to ensure uniqueness).
    * for each such pair, take a year of weather observations and determine the difference in temperature measurements taken at the same hour
        * HashMap (replicated) join of station-station pairs on the observations table. You could also do a total sort of the pairs table and use a merge-join if you're memory constrained.
        * join the resulting table back onto the observations table.
        * (In this case, most weather stations are a part of at least one pair, and so most of the rows in the observations table are retained. If that weren't the case,
            * if most elements on the left are also not elements on the right, do a second semi-join to filter for observations that are on the right of some pair. That is, one join to get the observation-pairs and a second HashMap (fragment-replicate) join on ids that are the right member of some pair. (Most people on an auction site are buyers or sellers, though a very few are both.) (?if the pairs were from airports to not-airports)
            * if as in this case, most weather stations are reasonably likely to be on either side, do a semi-join of observations against all distinct ids that are on either side of a pair. This means a single HashMap join against he huge table and then
            * Cogroup observations with id_a-sorted-pairs by id_a, id_b-sorted-pairs on id_b. (Preparing the tables in sorted form lets you use merge. Flatten (left_id, right_id, temp, distance) to get the left-observations. Filter for rows with at least one right id and project `(id_b, temp)` to get the semi-join. Don't flatten for this second table -- you want to join this table with one row per observation to the table with one row per observation-pair.

    * As the radius expands, you'll quickly find that the amount of data begins to explode, so restrict that upper radius band initially.
    * (is this also a problem: "You might have also noticed another problem. Even apart from a distance effect, with more neighbors there are more opportunities for observations to disagree.")
    *
    * (you should know that the answer has some bias -- places with a large concentration of weather stations are typically heavily populated, and heavily populated places don't tend to have extreme weather. We're just looking for a good rule-of-thumb though)


==== Finding the Centroid of an Extent


==== Finding the Bounding Box of an Extent


==== Finding the Bounding Box of Points Within a Radius


==== Combining Regions with Set Operations

(intersection, union, diff, xor)

==== Testing the Relationship of two Regions

DE-9IM

equals
disjoint
touches
contains
covers

intersects,
within
covered_by

crosses
overlaps

From Wikipedia:

	Equals:   a = b    that is    (a ∩ b = a) ∧ (a ∩ b = b)
	Within:   a ∩ b = a
	Intersects:   a ∩ b ≠ ∅
	Touches:   (a ∩ b ≠ ∅) ∧ (aο ∩ bο = ∅)

	point/point	Equals, Disjoint	Other valid predicates collapses into Equals.
	point/line	adds Intersects	Intersects is a flexibilization of Equals, "some equal point at the line".
	line/line	adds Touches, Crosses, ...	Touches is a constraint of Intersects, about "only boundaries"; Crosses about "only one point".

{0,1,2,T,F,*} -- dimensions 0, 1, 2; T / F; dont-care



=== Key Strategic Pattern: Tile / Cull / Process


* _Tile_    -- tile the grid
* _Cull_    -- eliminate
* _Process_ --

=== Matching Points in a Table with Nearby Points in Another (Spatial Join)


* scatter points to nine tiles


=== Matching Points with the Regions




=== Mechanics of Geographic Data

==== Longitude and Latitude, Points and Features

* floating point vs decimal -- The level of precision we're working with here doesn't justify giving up the benefits of a direct representation.

==== GeoJSON


* OpenDataLab POJOs for Jackson
  - https://github.com/opendatalab-de/geojson-jackson

* GeoTools http://www.geotools.org/
  -

* ESRI library:
  - https://github.com/Esri/geometry-api-java -- The Esri Geometry API for Java enables developers to write custom applications for analysis of spatial data. This API is used in the Esri GIS Tools for Hadoop and other 3rd-party data processing solutions.
  - https://github.com/Esri/spatial-framework-for-hadoop
*

=== Spatial Nearest-Feature Join


You might not expect it, but it can be more complex to match shapes with their _nearest_ point than to match shapes with _all nearby_ points.


In New York City, you'd be disappointed to learn that the nearest Starbucks was more than a few blocks away; in rural Montana, you'd be pleased to learn that one opened up less than an hour's drive away. http://www.ifweassume.com/2012/10/the-united-states-of-starbucks.html
Naively working at the coarse grain of Montana will pour every coffee joint in the big apple onto the same reducer, naively working on the fine grain of New York City will split Montana into a wasteful number of empty fragments that contain no coffee shops at all.
There's a wonderful tool we can borrow from our mathematician friends called a


==== Voronoi Cells

How do we extend region of a
How would you help find the nearest 7-11?
  -- one way would be to look for stores within X distance,
  but a customer in the western US might be excited to learn there's one within an hour's drive,
  while that same radius centered on Manhattan would require sorting through thousande

==== Breaking Regions into Quad Cells

* recursively decompose a region on quadcells

(what do numbers look like doing this for the US, daily)



==== Comparing Distributions ====

We now have a set of `[place, sighting]` pairs, and we want to understand how the distribution of coincidences compares to the background distribution of places.

(TODO: don't like the way I'm currently handling places near multiple sightings)

That is, we will compare the following quantities:

    count of sightings
    count of features
    for each feature type, count of records
    for each feature type, count of records near a sighting

The dataset at this point is small enough to do this locally, in R or equivalent; but if you're playing along at work your dataset might not be. So let's use pig.

    place_sightings = LOAD "..." AS (...);

    features = GROUP place_sightings BY feature;

    feature_stats = FOREACH features {
      sighted = FILTER place_sightings BY sighted;
      GENERATE features.feature_code,
        COUNT(sighted)      AS sighted_count,
	COUNT_STAR(sighted) AS total_count
	;
    };

    STORE feature_stats INTO '...';

results:

    ... TODO move results over from cluster ...


==== Exploration

* _Exemplars_
  - Tokyo
  - San Francisco
  - The Posse East Bar in Austin, TX footnote:[briefly featured in the Clash's Rock the Casbah Video and where much of this book was written]


===== Quadkey to and from Longitude/Latitude =====

    # converts from even/odd state of tile x and tile y to quadkey. NOTE: bit order means y, x
    BIT_TO_QUADKEY = { [false, false] => "0", [false, true] => "1", [true, false] => "2", [true, true] => "3", }
    # converts from quadkey char to bits. NOTE: bit order means y, x
    QUADKEY_TO_BIT = { "0" => [0,0], "1" => [0,1], "2" => [1,0], "3" => [1,1]}

    # Convert from tile x,y into a quadkey at a specified zoom level
    def tile_xy_zl_to_quadkey(tile_x, tile_y, zl)
      quadkey_chars = []
      tx = tile_x.to_i
      ty = tile_y.to_i
      zl.times do
        quadkey_chars.push BIT_TO_QUADKEY[[ty.odd?, tx.odd?]] # bit order y,x
        tx >>= 1 ; ty >>= 1
      end
      quadkey_chars.join.reverse
    end

    # Convert a quadkey into tile x,y coordinates and level
    def quadkey_to_tile_xy_zl(quadkey)
      raise ArgumentError, "Quadkey must contain only the characters 0, 1, 2 or 3: #{quadkey}!" unless quadkey =~ /\A[0-3]*\z/
      zl = quadkey.to_s.length
      tx = 0 ; ty = 0
      quadkey.chars.each do |char|
        ybit, xbit = QUADKEY_TO_BIT[char] # bit order y, x
        tx = (tx << 1) + xbit
        ty = (ty << 1) + ybit
      end
      [tx, ty, zl]
    end

==== Working with paths ====

The _smallest tile that fully encloses a set of points_ is given by the tile with the largest common quadtile prefix. For example, the University of Texas (quad `0231_3012_0331_1131`) and my office (quad `0231_3012_0331_1211`) are covered by the tile `0231_3012_0331_1`.

image::images/fu05-geographic-path-hq-to-ut.png[Path from Chimp HQ to UT campus]

When points cross major tile boundaries, the result is less pretty. Austin's airport (quad `0231301212221213`) shares only the zoom-level 8 tile `02313012`:

image::images/fu05-geographic-path-hq-to-airport.png[Path from Chimp HQ to AUS Airport]

==== Calculating Distances ====

To find the distance between two points on the globe, we use the Haversine formula


in code:

    # Return the haversine distance in meters between two points
    def haversine_distance(left, top, right, btm)
      delta_lng = (right - left).abs.to_radians
      delta_lat = (btm   - top ).abs.to_radians
      top_rad = top.to_radians
      btm_rad = btm.to_radians

      aa = (Math.sin(delta_lat / 2.0))**2 + Math.cos(top_rad) * Math.cos(btm_rad) * (Math.sin(delta_lng / 2.0))**2
      cc = 2.0 * Math.atan2(Math.sqrt(aa), Math.sqrt(1.0 - aa))
      cc * EARTH_RADIUS
    end

    # Return the haversine midpoint in meters between two points
    def haversine_midpoint(left, top, right, btm)
      cos_btm   = Math.cos(btm.to_radians)
      cos_top   = Math.cos(top.to_radians)
      bearing_x = cos_btm * Math.cos((right - left).to_radians)
      bearing_y = cos_btm * Math.sin((right - left).to_radians)
      mid_lat   = Math.atan2(
        (Math.sin(top.to_radians) + Math.sin(btm.to_radians)),
        (Math.sqrt((cos_top + bearing_x)**2 + bearing_y**2)))
      mid_lng   = left.to_radians + Math.atan2(bearing_y, (cos_top + bearing_x))
      [mid_lng.to_degrees, mid_lat.to_degrees]
    end

    # From a given point, calculate the point directly north a specified distance
    def point_north(longitude, latitude, distance)
      north_lat = (latitude.to_radians + (distance.to_f / EARTH_RADIUS)).to_degrees
      [longitude, north_lat]
    end

    # From a given point, calculate the change in degrees directly east a given distance
    def point_east(longitude, latitude, distance)
      radius = EARTH_RADIUS * Math.sin(((Math::PI / 2.0) - latitude.to_radians.abs))
      east_lng = (longitude.to_radians + (distance.to_f / radius)).to_degrees
      [east_lng, latitude]
    end

===== Grid Sizes and Sample Preparation =====

Always include as a mountweazel some places you're familiar with. It's much easier for me to think in terms of the distance from my house to downtown, or to Dallas, or to New York than it is to think in terms of zoom level 14 or 7 or 4

==== Distributing Boundaries and Regions to Grid Cells ====

(TODO: Section under construction)

This section will show how to

* efficiently segment region polygons (county boundaries, watershed regions, etc) into grid cells
* store data pertaining to such regions in a grid-cell form: for example, pivoting a population-by-county table into a population-of-each-overlapping-county record on each quadtile.

==== Reducer: combine objects on each quadtile ====

The reducer is now fairly simple. Each quadtile will have a handful of UFO sightings, and a potentially large number of geonames places to test for nearbyness. The nearbyness test is straightforward:

	# from wukong/geo helpers

        class BoundingBox
          def contains?(obj)
	    ( (obj.longitude >= left)  && (obj.latitude <= top) &&
	      (obj.longitude <= right) && (obj.latitude >= btm)
	  end
	end

	# nearby_ufos.rb

	class NearbyReducer

	  def process_group(group)
	    # gather up all the sightings
	    sightings = []
	    group.gather(UfoSighting) do |sighting|
              sightings << sighting
            end
	    # the remaining records are places
	    group.each do |place|
	      sighted = false
	      sightings.each do |sighting|
	        if sighting.contains?(place)
		  sighted = true
		  yield combined_record(place, sighting)
		end
              end
	      yield unsighted_record(place) if not sighted
	    end
	  end

	  def combined_record(place, sighting)
	    (place.to_tuple + [1] + sighting.to_tuple)
	  end
	  def unsighted_record(place)
	    place.to_tuple + [0]
	  end
	end

For now I'm emitting the full place and sighting record, so we can see what's going on. In a moment we will change the `combined_record` method to output a more disciplined set of fields.

Output data:

        ...


==== Map Polygons to Grid Tiles



              +----------------------------+
              |                            |
              |              C             |
              |      ~~+---------\         |
              |     /  |          \       /
              |    /   |           \     /|
              |   /    |            \   / |
               \ /     |     B       \ /  |
                |      |              |   |
                |  A   +--------------'   |
                |      |                  |
                |      |     D            /
                |      |               __/
                 \____/ \             |
                         \____________,


            +-+-----------+-------------+--+------
            | |           |             |  |
            | |           |         C   |  |
      000x  | |   C  ~~+--+------\      |  |      0100
            | |     / A|B |  B    \     | /
            |_|____/___|__|________\____|/|_______
            | | C /    |  |         \ C / |
            |  \ /     |B |  B       \ /| |
      001x  |   |      |  |           | |D|       0110
            |   |  A   +--+-----------' | |
            |   |      |D |  D          | |
            +---+------+--+-------------+-/-------
            |   |  A   |D |            _|/
            |    \____/ \ |    D      | |
      100x  |            \|___________, |         1100
            |             |             |
            |             |             |
            +-------------+-------------+---------
                ^ 1000        ^ 1001

* Tile 0000: `[A, B, C   ]`
* Tile 0001: `[   B, C   ]`
* Tile 0010: `[A, B, C, D]`
* Tile 0011: `[   B, C, D]`

* Tile 0100: `[      C,  ]`
* Tile 0110: `[      C, D]`

* Tile 1000: `[A,       D]`
* Tile 1001: `[         D]`
* Tile 1100: `[         D]`

For each grid, also calculate the area each polygon covers within that grid.

Pivot:

* A:          `[ 0000       0010                   1000          ]`
* B:          `[ 0000 0001 0010 0011                             ]`
* C:          `[ 0000 0001 0010 0011 0100 0110                   ]`
* D:          `[             0010 0011       0110 1000 1001 1100 ]`



==== Joining Stadiums onto Quad Cells

join games on parks_info to get location, quadkey

foreach stadiums to get months in action and quadkey
join stadiums by (quadkey, month), weather voronois by (quadkey, month)
find actual nearest weather station for that stadium

now park_info has month, quadkey, weather station, date, parkid
join park_info on games -- get game_wstns (game_id, scorecard weather, scorecard_wind, park_id, wstn_id)
join game_wstns on wobs to get game_weather


A bounding box around the

* Continental US: `-125.0011, 24.9493, -66.9326, 49.5904`; centroid: `-95.9669, 37.1669`.
* Alaska: `179.1506, 51.2097, -129.9795, 71.4410


=== Multi-Scale Spatial Data

If we want to combine weather

==== Breaking regions into Multi-Cell Quads


==== Adaptive Grid Size

The world is a big place, but we don't use all of it the same. Most of the world is water. Lots of it is Siberia. Half the tiles at zoom level 2 have only a few thousand inhabitantsfootnote:[000 001 100 101 202 203 302 and 303].

Suppose you wanted to store a "what country am I in" dataset -- a geo-joinable decomposition of the region boundaries of every country. You'll immediately note that Monaco fits easily within on one zoom-level 12 quadtile; Russia spans two zoom-level 1 quadtiles. Without multiscaling, to cover the globe at 1-km scale and 64-kB records would take 70 terabytes -- and 1-km is not all that satisfactory. Huge parts of the world would be taken up by grid cells holding no border that simply said "Yep, still in Russia".

There's a simple modification of the grid system that lets us very naturally describe multiscale data.

The figures (REF: multiscale images) show the quadtiles covering Japan at ZL=7. For reasons you'll see in a bit, we will split everything up to at least that zoom level; we'll show the further decomposition down to ZL=9.

image::images/fu05-quadkeys-multiscale-ZL7.png[Japan at Zoom Level 7]

Already six of the 16 tiles shown don't have any land coverage, so you can record their values:

    1330000xx  { Pacific Ocean }
    1330011xx  { Pacific Ocean }
    1330013xx  { Pacific Ocean }
    1330031xx  { Pacific Ocean }
    1330033xx  { Pacific Ocean }
    1330032xx  { Pacific Ocean }

Pad out each of the keys with `x`'s to meet our lower limit of ZL=9.

The quadkey `1330011xx` means "I carry the information for grids `133001100`, `133001101`, `133001110`, `133001111`, ".

image::images/fu05-quadkeys-multiscale-ZL8.png[Japan at Zoom Level 8]

image::images/fu05-quadkeys-multiscale-ZL9.png[Japan at Zoom Level 9]

You should uniformly decompose everything to some upper zoom level so that if you join on something uniformly distributed across the globe you don't have cripplingly large skew in data size sent to each partition.  A zoom level of 7 implies 16,000 tiles -- a small quantity given the exponential growth of tile sizes

With the upper range as your partition key, and the whole quadkey is the sort key, you can now do joins. In the reducer,

* read keys on each side until one key is equal to or a prefix of the other.
* emit combined record using the more specific of the two keys
* read the next record from the more-specific column,  until there's no overlap

Take each grid cell; if it needs subfeatures, divide it else emit directly.

You must emit high-level grid cells with the lsb filled with XX or something that sorts after a normal cell; this means that to find the value for a point,

* Find the corresponding tile ID,
* Index into the table to find the first tile whose ID is larger than the given one.

     00.00.00
     00.00.01
     00.00.10
     00.00.11
     00.01.--
     00.10.--
     00.11.00
     00.11.01
     00.11.10
     00.11.11
     01.--.--
     10.00.--
     10.01.--
     10.10.01
     10.10.10
     10.10.11
     10.10.00
     10.11.--

==== Tree structure of Quadtile indexing

You can look at quadtiles is as a tree structure. Each branch splits the plane exactly in half by area, and only leaf nodes hold data.

The first quadtile scheme required we develop every branch of the tree to the same depth. The multiscale quadtile scheme effectively says "hey, let's only expand each branch to its required depth". Our rule to break up a quadtile if any section of it needs development preserves the "only leaf nodes hold data". Breaking tiles always exactly in two makes it easy to assign features to their quadtile and facilitates joins betweeen datasets that have never met. There are other ways to make these tradeoffs, though -- read about K-D trees in the "keep exploring" section at end of chapter.



* _choose exemplars_:
  - Midway, because it's large; Austin, because it's one of our exemplar cities; and (TODO something tiny) because it's very small.
  - the sightings X, y, which each have a fun description and are near multiple airports; and Z, which is not near an airport.
  - weather observations:
      - a date with a new moon and a full moon; 8/8/08, because auspicious; an equinox and a solstice
  -

What makes a good exemplar?
* Head-of-the-tail --
    * extreme specimens will pop on their own. You want to see what's happening to the
* Ones that are unusual without being weird. The solstice is
* Essential troublemakers: leap years, the centennial leap-year-exceptions, and the quad-centennial leap-year-exception-exceptions.
* Well represented
    * it's no fun if your exemplars disappear mid-journey -- most commonly because they failed to find a match during a join.
* Chosen by out-of-band criteria -- deciding to look for "this date three years ago" and then finding a record is better than choosing the first record you see -- that particular record may have been the first one you saw because it is unrepresentative in some way.
    * just as a magician will pull back their shirtsleeves to show they have no rabbit concealed within, this keeps you from fooling yourself. http://en.wikipedia.org/wiki/Nothing_up_my_sleeve_number
    * (in fact, Cryptographers have a concept of a "nothing-up-my-sleeve" number: when a large arbitrary collection of numbers is needed, choosing the first twenty-five digits of Pi is believably arbitrary, whereas choosing the 387'th through 412'th digits raises the specter of a purposeful "backdoor").


==== Finding Nearby Objects

Let’s use the GeoNames dataset to create a “nearest <whatever> to you” application, one that, given a visitor’s geolocation, will return the closest hospital, school, restaurant and so forth.  We will do so by effectively pre-calculating all potential queries; this could be considered overkill for the number of geofeatures within the GeoNames dataset but we want to illustrate an approach that will scale to the number of cell towers, gas stations or anything else.

We will not go into the details of computing a decomposition; most scientific computing libraries have methods to do so and we have included a Python script (TODO: credits), which, when fed a set of locations, returns a set of GeoJSON regions, the Voronoi polygon for each location.

Run the script 'examples Geo Voronoi points to polygons.pi' (TODO: fix up command line).  After a few minutes, it will produce 'output GeoJSON' files.  To see the output (TODO: give instructions for seeing it in browser).

These polygons are pretty but not directly useful; we need a way to retrieve the relevant polygons for a given visitor’s location.  What we will do is store, for every quad key, the truncated Voronoi regions that lie within its quad tile.  We can then turn the position of a visitor into its corresponding quad key, retrieve the set of regions on that quad tile and find the specific region within which it lies.

Pig does not have any built-in geospatial features, so we will have to use a UDF.  In fact, we will reach into the future and use one of the ones you will learn about in the Advanced Pig chapter (TODO:  REF). Here is the script to

----
Register the UDF
Give it an alias
Load the polygons file
Turn each polygon into a bag of quad key polygon metadata tuples
Group by quad key
FOREACH generate the output data structure
Store results
----

Transfer the output of the Voronoi script onto the HDFS and run the above Pig script.  Its output is a set of TSV files in which the first column is a quad key and the second column is a set of regions in GeoJSON format.  We will not go into the details, but the example code shows how to use this to power the nearest x application.  Follow the instructions to load the data into HBase and start the application.

The application makes two types of requests:  One is to determine which polygon is the nearest; it takes the input coordinates and uses the corresponding quad tile to retrieve the relevant regions.  It then calls into a geo library to determine which polygon contains the point and sends a response containing the GeoJSON polygon.  The application also answers direct requests for a quad tile with a straight GeoJSON stored in its database -- exactly what is required to power the drivable "slippy map" widget that is used on the page.  This makes the front end code simple, light and fast, enough that mobile devices will have no trouble rendering it.  If you inspect the Javascript file, in fact, it is simply the slippy map's example with the only customization being the additional query for the region of interest.  It uses the server's response to simply modify the style sheet rule for that portion of the map.

The same data locality advantages that the quad key scheme grants are perhaps even more valuable in a database context, especially ones like HBase that store data in sorted form.  We are not expecting an epic storm of viral interest in this little app but you might be for the applications you write.

The very thing that makes such a flood difficult to manage -- the long-tail nature of the requests -- makes caching a suitable remedy.  You will get a lot more repeated requests for downtown San Francisco than you will for downtown Cheboygan, so those rows will always be hot in memory.  Since those points of lie within compact spatial regions, they also lie within not many more quad key regions, so the number of database blocks contending for cache space is very much smaller than the number of popular quad keys.

It also addresses the short-tail caching problem as well.  When word does spread to Cheboygan and the quad tile for its downtown is loaded, you can be confident requests for nearby tiles driven by the slippy map will follow as well.  Even if those rows are not loaded within the same database block, the quad key helps the operating system pick up the slack -- since this access pattern is so common, when a read causes the OS to go all the way to disk, it optimistically pre-fetches not just the data you requested but a bit of what follows.  When the database gets around to loading a nearby database block, there is a good chance the OS will have already buffered its contents.

The strategies employed here -- precalculating all possible requests, identifying the nature of popular requests, identifying the nature of adjacent requests and organizing the key space to support that adjacency -- will let your database serve large-scale amounts of data with millisecond response times even under heavy load.

.Sidebar:  Choosing A Decomposition Zoom Level
----
When you are decomposing spatial data onto quad tiles, you will face the question of what zoom level or zoom levels to choose.  At some point, coarser (lower indexed) zoom levels will lead to overpopulated tiles, tiles whose record size is unmanageably large; depending on your dataset, this could happen at zoom level 9 (the size of outer London), zoom level 12 (the size of Manhattan south of Central Park) or even smaller.  At the other end, finer zoom levels will produce unjustifiably many boring or empty tiles.

To cover the entire globe at zoom level 13 requires 67 million records, each covering about four kilometers; at zoom level 16, you will need four billion records, each covering about a half kilometer on a side; at zoom level 18, you will need 69 billion records, each covering a city block or so.  To balance these constraints, build a histogram of geofeature counts per quad tile at various zoom levels.  Desirable zoom levels are such that the most populous bin will have acceptable size while the number of bins with low geofeature count are not unmanageably numerous.  Quad keys up to zoom level 16 will fit within a 32-bit unsigned integer; the improved efficiency of storage and computation make a powerful argument for using zoom levels 16 and coarser, when possible.
----

==== Break polygons on quadtiles

Now let's put the diagram to work. Use the weather station locations to define a set of Voronoi polygons, treating each weather station's observations as applying uniformly to the whole of that polygon.

Break the Voronoi polygons up by quadtile as we did above -- quadtiles will either contain a piece of boundary (and so are at the lower-bound zoom level), or are entirely contained within a boundary. You should choose a lower-bound zoom level that avoids skew but doesn't balloon the dataset's size.

Also produce the reverse mapping, from weather station to the quadtile IDs its polygon covers.

==== Map Observations to Grid Cells

Now join observations to grid cells and reduce each grid cell.

==== Voronoi Polygons turn Points into Regions

Now, let's use the Voronoi trick to turn a distribution of measurements at discrete points into the distribution over regions it is intended to represent.  In particular, we will take the weather-station-by-weather-station measurements in the NCDC dataset and turn it into an hour-by-hour map of global data.  Spatial distribution of weather stations varies widely in space and over time; for major cities in recent years, there may be many dozens while over stretches of the Atlantic Ocean and in many places several decades ago, weather stations might be separated by hundreds of miles.  Weather stations go in and out of service, so we will have to prepare multiple Voronoi maps.  Even within their time of service, however, they can also go offline for various reasons, so we have to be prepared for missing data.  We will generate one Voronoi map for each year, covering every weather station active within that year, acknowledging that the stretch before and after its time of service will therefore appear as missing data.

In the previous section, we generated the Voronoi region because we were interested in its seed location.  This time, we are generating the Voronoi region because we are interested in the metadata that seed location imputes.  The mechanics are otherwise the same, though, so we will not repeat them here (they are described in the example codes documentation (TODO:  REF).

At this point, what we have are quad tiles with Voronoi region fragments, as in the prior example, and we could carry on from there.  However, we would be falling into the trap of building our application around the source data and not around the user and the application domain.  We should project the data onto regions that make sense for the domain of weather measurements not regions based on where it is convenient to erect a weather vane.

The best thing for the user would be to choose a grid size that matches the spatial extent of weather variations and combine the measurements its weather stations into a consensus value; this will render wonderfully as a heat map of values and since each record corresponds to a full quad cell, will be usable directly by downstream analytics or applications without requiring a geospatial library.  Consulting the quad key grid size cheat sheet (TODO:  REF), zoom level 12 implies 17 million total grid cells that are about five to six miles on a side in populated latitudes, which seems reasonable for the domain.

As such, though, it is not reasonable for the database.  The dataset has reasonably global coverage going back at least 50 years or nearly half a million hours.  Storing 1 KB of weather data per hour at zoom-level 12 over that stretch will take about 7.5 PB but the overwhelming majority of those quad cells are boring.  As mentioned, weather stations are sparse over huge portions of the earth.  The density of measurements covering much of the Atlantic Ocean would be well served by zoom-level 7; at that grid coarseness, 50 years of weather data occupies a mere 7 TB; isn't it nice to be able to say a "mere" 7 TB?

What we can do is use a multi-scale grid.  We will start with a coarsest grain zoom level to partition; 7 sounds good.  In the Reducers (that is, after the group), we will decompose down to zoom-level 12 but stop if a region is completely covered by a single polygon.  Run the multiscale decompose script (TODO: demonstrate it).  The results are as you would hope for; even the most recent year's map requires only x entries and the full dataset should require only x TB.

The stunningly clever key to the multiscale JOIN is, well, the keys.  As you recall, the prefixes of a quad key (shortening it from right to left) give the quad keys of each containing quad tile.  The multiscale trick is to serialize quad keys at the fixed length of the finest zoom level but where you stop early to fill in with an '.' - because it sorts lexicographically earlier than the numerals do.  This means that the lexicographic sort order Hadoop applies in the midstream group-sort still has the correct spatial ordering just as Zorro would have it.

Now it is time to recall how a JOIN works covered back in the Map/Reduce Patterns chapter (TODO:  REF).  The coarsest Reduce key is the JOIN value, while the secondary sort key is the name of the dataset.  Ordinarily, for a two-way join on a key like 012012, the Reducer would buffer in all rows of the form <012012 | A | ...>, then apply the join to each row of the form <012012 | B | ...>.  All rows involved in the join would have the same join key value.  For a multiscale spatial join, you would like rows in the two datasets to be matched whenever one is the same as or a prefix of the other.  A key of 012012 in B should be joined against a key of `0120..`, '01201.' and '012012' but not, of course, against '013...'.

We can accomplish this fairly straightforwardly.  When we defined the multiscale decomposition, we a coarsest zoom level at which to begin decomposing and the finest zoom level which defined the total length of the quad key.  What we do is break the quad key into two pieces; the prefix at the coarsest zoom level (these will always have numbers, never dots) and the remainder (fixed length with some number of quad key digits then some number of dots).  We use the quad key prefix as the partition key with a secondary sort on the quad key remainder then the dataset label.

Explaining this will be easier with some concrete values to use, so let's say we are doing a multiscale join between two datasets partitioning on a coarsest zoom level of 4, and a total quad key length of 6, leading to the following snippet of raw reducer input.

.Snippet of Raw Reducer Input for a Multiscale Spatial Join
----
0120    1.   A
0120    10   B
0120    11   B
0120    12   B
0120    13   B
0120    2.   A
0120    30   B
0121    00   A
0121    00   B
----

As before, the reducer buffers in rows from A for a given key -- in our example, the first of these look like <0120 | 1. | A | ...>. It will then apply the join to each row that follows of the form <0120 | (ANYTHING) | B | ...>.  In this case, the 01201. record from A will be joined against the 012010, 012011, 012012 and 012013 records from B.  Watch carefully what happens next, though.  The following line, for quad key 01202. is from A and so the Reducer clears the JOIN buffer and gets ready to accept records from B to join with it.  As it turns out, though, there is no record from B of the form 01202-anything.  In this case, the 01202. key from A matches nothing in B and the 012030 key in B is matched by nothing in A (this is why it is important the replacement character is lexicographically earlier than the digits; otherwise, you would have to read past all your brothers to find out if you have a parent).  The behavior is the same as that for a regular JOIN in all respects but the one, that JOIN keys are considered to be equal whenever their digit portions match.

The payoff for all this is pretty sweet.  We only have to store and we only have to ship and group-sort data down to the level at which it remains interesting in either dataset.  When the two datasets meet in the Reducer, the natural outcome is as if they were broken down to the mutually-required resolution.  The output is also efficiently multiscale.

==== Smoothing the Distribution

We now have in hand, for each year, a set of multiscale quad tile records with each record holding the weather station IDs that cover it.  What we want to produce is a dataset that has, for each hour and each such quad tile, a record describing the consensus weather on that quad tile.  If you are a meteorologist, you will probably want to take some care in forming the right weighted summarizations -- averaging the fields that need averaging, thresholding the fields that need thresholding and so forth.  We are going to cheat and adopt the consensus rule of "eliminate weather stations with missing data, then choose the weather station with the largest area coverage on the quad tile and use its data unmodified."  To assist that, we made a quiet piece of preparation and have sorted the weather station IDs from largest to smallest in area of coverage, so that the Reducer simply has to choose from among its input records the earliest one on that list.

What we have produced is gold dataset useful for any number of explorations and applications.  An exercise at the end of the chapter (TODO:  REF) prompts you to make a visual browser for historical weather.  Let's take it out for a simple analytical test drive, though.

The tireless members of Retrosheet.org have compiled box scores for nearly every Major League Baseball game since its inception in the late 1800s.  Baseball score sheets typically list the game time weather and wind speed and those fields are included in the Retrosheet data; however, values are missing for many records and since this is hand-entered data, surely many records have coding errors as well.  For example, on October 1, 2006, the home-team Brewers pleased a crowd of 44,133 fans with a 5-3 win over the Cardinals on a wonderful fall day recorded as having game-time temperature of 83 degrees, wind 60 miles per hour out to left field and sunny.  In case you are wondering, 60-mile per hour winds cause 30-foot waves at sea, trees to be uprooted and structural damage to buildings becomes likely, so it is our guess that the scoresheet is, in this respect, wrong.

Let's do a spatial drawing of the Retrosheet data for each game against the weather estimated using the NCDC dataset for that stadium's location at the start of the game; this will let us fill in missing data and flag outliers in the Retrosheet scores.

Baseball enthusiasts are wonderfully obsessive, so it was easy to find online data listing the geographic location of every single baseball stadium -- the file sports/baseball/stadium_geolocations.tsv lists each Retrosheet stadium ID followed by its coordinates and zoom-level 12 quad key.  Joining that on the Retrosheet game logs equips the game log record with the same quad key and hour keys used in the smoothed weather dataset.  (Since the data is so small, we turned parallelism down to 1.)

Next, we will join against the weather data; this data is so large, it is worth making a few optimizations.  First, we will apply the guideline of "join against the smallest amount of data possible."  There are fewer than a hundred quad keys we are interested in over the whole time period of interest and the quad key breakdown only changes year by year, so rather than doing a multiscale join against the full hourly record, we will use the index that gives the quad key breakdown per year to find the specific containing quad keys for each stadium over time.  For example (TODO: find an example where a quad key was at a higher zoom level one year and a lower one a different year).  Doing the multiscale join of stadium quad keys against the weather quad key year gives (TODO: name of file).

Having done the multiscale join against the simpler index, we can proceed using the results as direct keys; no more multiscale magic is required.  Now that we know the specific quad keys and hours, we need to extract the relevant weather records.  We will describe two ways of doing this.  The straightforward way is with a join, in this case of the massive weather quad tile data against the relatively tiny set of quad key hours we are interested in.  Since we do not need multiscale matching any more, we can use Pig and Pig provides a specialized join for the specific case of joining a tiny dataset to a massive one, called the replicated join.  You can skip ahead to the Advanced Pig chapter (TODO:  REF) to learn more about it; for now, all you need to know is that you should put the words "`USING 'replicated'`" at the end of the line, and that the smallest dataset should be on the _right_. (Yes, it's backwards: for replicated joins the smallest should be on the right, while for regular joins it should be on the left.)  This type of join loads the small dataset into memory and simply streams through the larger dataset, so no Reduce is necessary.  It's always a good thing when you can avoid streaming TB of data through the network card when all you want are a few MB.

In this case, there are a few thousand lines in the small dataset, so it is reasonable to do it the honest way, as just described.  In the case where you are just trying to extract a few dozen keys, your authors have been known to cheat by inlining the keys in a filter.  Regular expression engines are much faster than most people realize and are perfectly content to accept patterns with even a few hundred alternations.  An alternative approach here is to take the set of candidate keys, staple them together into a single ludicrous regexp and template it into the PIg script you will run.

.Cheat to Win: Filtering down to only joinable keys using a regexp
----
huge_data = LOAD '...' AS f1, f2, f3;
filtered_data = FILTER huge_data BY MATCH(f1, '^(012012|013000|020111| [...dozens more...])$');
STORE filtered_data INTO '...';
----

==== Results

With just the relevant records extracted, we can compare the score sheet data with the weather data.  Our script lists output columns for the NCDC weather and wind speed, the score sheet weather and wind speed, the distance from the stadium to the relevant weather station and the percentage difference for wind speed and temperature.

// It would be an easy mistake to, at this point, simply evict the Retrosheet measurements and replace with the NCDC measurements; we would not argue for doing so.  First, the weather does vary, so there is some danger in privileging the measurement at a weather station some distance away (even if more precise) over a direct measurement at a correct place and time.  In fact, we have far better historical coverage of the baseball data than the weather data.  The weather data we just prepared gives a best-effort estimate of the weather at every quad tile, leaving it in your hands to decide whether to accept a reading from a weather station dozens or hundreds of miles away.  Rather, the philosophically sound action would be to flag values for which the two datasets disagree as likely outliers.

// The successful endpoint of most Big Data explorations is a transition to traditional statistical packages and elbow grease -- it shows you've found domain patterns worth exploring.
// If this were a book about baseball or forensic econometrics, we'd carry forward comparing those outliers with local trends, digging up original entries, and so forth.  Instead, we'll just label them with a scarlet "O" for outlier, drop the mic and walk off stage.

=== Conclusions

Most importantly of all, this chapter will solidify your intuition about how to move and combine data strategically.  You'll be learning new types of joins that require a more sophistitcated notion of 'context' (data locality).  But the deep natural intuition about space and physical locality you already possess makes this challenge much easier.  Most readers should concentrate foremost on the tactics of applying our analytic patterns. (In fact, if you want to trust us that the sections where we dive into the underlying map/reduce work as advertised, we've made them easy to skip the first time through.)  But come back at some point and step through the spatial and multi-scale spatial joins we introduce in this chapter.  If you understand the underlying mechanics -- how the design of the keys ensures records that must be related in context show up at the right place in the right order -- then you have mastered the deep concept of map/reduce.


=== Keep Exploring ===

==== Balanced Quadtiles =====

Earlier, we described how quadtiles define a tree structure, where each branch of the tree divides the plane exactly in half and leaf nodes hold features. The multiscale scheme handles skewed distributions by developing each branch only to a certain depth. Splits are even, but the tree is lopsided (the many finer zoom levels you needed for New York City than for Irkutsk).

K-D trees are another approach. The rough idea: rather than blindly splitting in half by area, split the plane to have each half hold the same-ish number of points. It's more complicated, but it leads to a balanced tree while still accommodating highly-skew distributions. Jacob Perkins (`@thedatachef`) has a http://thedatachef.blogspot.com/2012/10/k-d-tree-generation-with-apache-pig.html[great post about K-D trees] with further links.

==== It's not just for Geo =====

=== Exercises ===

[[brain_example]]
**Exercise 1**: Extend quadtile mapping to three dimensions

To jointly model network and spatial relationship of neurons in the brain, you will need to use not two but three spatial dimensions. Write code to map positions within a 200mm-per-side cube to an "octcube" index analogous to the quadtile scheme. How large (in mm) is each cube using 30-bit keys? using 63-bit keys?

For even higher dimensions of fun, extend the http://en.wikipedia.org/wiki/Voronoi_diagram#Higher-order_Voronoi_diagrams[Voronoi diagram to three dimensions].

**Exercise 2**: Locality

We've seen a few ways to map feature data to joinable datasets. Describe how you'd join each possible pair of datasets from this list (along with the story it would tell):

* Census data: dozens of variables, each attached to a census tract ID, along with a region polygon for each census tract.
* Cell phone antenna locations: cell towers are spread unevenly, and have a maximum range that varies by type of antenna.
  - case 1: you want to match locations to the single nearest antenna, if any is within range.
  - case 2: you want to match locations to all antennae within range.
* Wikipedia pages having geolocations.
* Disease reporting: 60,000 points distributed sparsely and unevenly around the country, each reporting the occurence of a disease.

For example, joining disease reports against census data might expose correlations of outbreak with ethnicity or economic status. I would prepare the census regions as quadtile-split polygons. Next, map each disease report to the right quadtile and in the reducer identify the census region it lies within. Finally, join on the tract ID-to-census record table.

**Exercise 3**: Write a generic utility to do multiscale smoothing

Its input is a uniform sampling of values: a value for every grid cell at some zoom level.
However, lots of those values are similar.
Combine all grid cells whose values lie within a certain tolerance into

Example: merge all cells whose contents lie within 10% of each other

    00	10
    01	11
    02   9
    03   8
    10  14
    11  15
    12  12
    13  14
    20  19
    21  20
    22  20
    23  21
    30  12
    31  14
    32   8
    33   3

    10  11  14  18     .9.5. 14  18
     9   8  12  14     .   . 12  14
    19  20  12  14     . 20. 12  14
    20  21   8   3     .   .  8   3


    
