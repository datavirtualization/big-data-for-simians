== Storm+Trident Internals

// (TODO: check that Java classes and variables have the right capitalization)

What should you take away from this chapter:

You should:

* Understand the lifecycle of a Storm tuple, including spout, tupletree and acking.
* (Optional but not essential) Understand the details of its reliability mechanism and how tuples are acked.
* Understand the lifecycle of partitions within a Trident batch and thus, the context behind partition operations such as Apply or PartitionPersist.
* Understand Trident's transactional mechanism, in the case of a PartitionPersist.
* Understand how Aggregators, Statemap and the Persistence methods combine to give you _exactly once_  processing with transactional guarantees.  Specifically, what an OpaqueValue record will look like in the database and why.
* Understand how the master batch coordinator and spout coordinator for the Kafka spout in particular work together to uniquely and efficiently process all records in a Kafka topic.
* One specific:  how Kafka partitions relate to Trident partitions.

=== Storm tuple lifecycle

Once the Coordinator has brought the dataflow online, the Worker (TODO:  ?Executor?) calls the spouts next tuple operator.  The spout emits a tuple to its Collector if it has one ready.  (Actually, the spout is permitted to emit 0, 1 or _many tuples_ -- but you should try to emit just one unless there's a good reason.)  It then registers that tuple as pending until its tupletree (this tuple and all the descendent tuples produced by later processing stages) are acked.  Please note that though the spout pending mechanism relies on the final result of the acking mechanism, it is distinct from that and handled by the spout's Executor.  (TODO:  Check)

If the spout doesn't emit a tuple, the Worker will sleep for a fixed number of milliseconds (by default, you can change the sleep policy).  Otherwise, the Worker will keep calling `nextTuple` until either its send queue is full (see below) or until there are `MAX_SPOUT_PENDING` or more tuples pending.

==== Spout send queue

The Collector places that tuple into the Executor's send queue.

Since it's important that the spout never block when emitting (if it did, critical bookkeeping tuples might get trapped, locking up the flow), a spout emitter keeps an "overflow buffer," and publishes as follows:

* if there are tuples in the overflow buffer add the tuple to it -- the queue is certainly full.
* otherwise, publish the tuple to the flow with the non-blocking call. That call will either succeed immediately ...
* or fail with an `InsufficientCapacityException`, in which case add the tuple to the overflow buffer.

==== Executor Queues

At this point, you see that the spout spins in an independent loop, emitting records to its Collector until one of its limits is hit.  We will pick up with the specific tuple in a moment but first, let's get a picture of how tuples move between Executors, locally and remotely.  Each Executor, whether bolt or spout, has both a send and a receive queue.  (For now, all you need to know about a bolt Executor is that it takes things from its receive queue, does stuff to them and puts them into the send queue.)

(TODO:  Insert information on what disrupter queue is and how wonderful it is)

When a tuple is emitted, the Collector places each into a slot in the send queue, once for each downstream Executor that will process it.  (The code, if you should find yourself reading it, doesn't distinguish the tuple as emitted and the copies used for sending.  It has to make these multiple copies so that each can be acked independently.)  These writes are done in a blocking fashion.  If the queue is full, the "writemethod" does not return until it has been swept; this means that, in turn, the Collectors emit and the Executors execute methods block as well, preventing the Executor from sending more records than downstream stages can process.

The worker sweeps each Executor's send queue on a tight loop.  Unless the downstream queue blocks, it repeats immediately (TODO:  Check).  Each sweep of the queue gathers all tuples, removing them into a temporary buffer.  Tuples to be handled by a local Executor are deposited directly into that Executor's receive queue.  All tuples destined for remote Executors are placed in the Worker's transfer queue in a single write, regardless of how many remote Executors there are.

A couple of notes:  First, send and receive queues are per Executor, while the transfer queue is shared by all Executors in the Worker.

When the Worker writes to a downstream queue, it deposits all records from that sweep into the queue in a bunch.  [FOOTNOTE:  I'm being careful to use the informal term "bunch" rather than "batch" because you'll never hear about these again.  A "batch" is a principal element for Trident, whereas we'll never talk about these again.]  So note that, while each slot in a send queue holds exactly one tuple, each slot in a receive or transfer queue can hold up to the `SEND_QUEUE` size amount of tuples.  (TODO:  Check variable's name)  This is important when you're thinking about memory usage.

===== Worker Transfer Queue

The Worker transport mechanism in turn sweeps the transfer queue and writes tuples over the network to each destination worker.  There are two transport mechanisms, using either the legacy ZeroMQ transport or the more recent Netty transport.  The ZeroMQ transport has been replaced due to both licensing issues and minor dissatisfaction with its performance in production.  The Netty transport is probably the better choice, although at time of writing (November 2013), it's received much less production exposure.

In our experience, the remaining details of the Worker-to-Worker transfer are fairly unimportant.  Records are sent over the network, accepted by the remote Worker and deposited into the relevant Executors' receive queues.  There is such a thing as a Worker receive queue; this is not a disruptor queue and not a piece of user-maintainable machinery; ignore it and leave its configuration alone.

===== Executor Receive Queues

As you may now guess, each Executor runs in its own loop, sweeping each receive queue and removing all hanging bunches.  Each of those tuples are handed, one after the other, to its bolt's `executemethod`.

So now, we can explain the backpressure mechanism of a Storm flow -- the reason that tuples don't pile up unmanageably in RAM at the slowest stage.  As mentioned, the `executemethod` won't return if anything downstream is blocking.  Since each call to execute is done in series, this, in turn, prevents the Executor from iterating through all the tuples in a sweep -- preventing it from beaconing a new sweep.  Tuples will begin accumulating in a blocked Executor's receive queue.  Ultimately, the Worker will, as well, become blocked writing to that receive queue, keeping it from sweeping upstream send queues.  This will continue all the way up the flow to the spout.  Finally, as we hinted at the start, once the spout's send queue is full, it will stop calling `nexttuple`, stop draining its source and so stop writing more records into the flow.

If this sounds awfully coarse-grained, you're right. While nothing will _break_ if you get to this L.A. freeway state of gridlock, your flow will have become disastrously inefficient, even well before that point. You can straightforwardly prevent the situation by adjusting the `maxspoutpending` parameter and each stage's parallelism correctly in the next chapter (TODO:  ref), Storm+Trident Tuning, will show you how.

In normal operation, you shouldn't have to think about the backpressure mechanics, though; Storm quietly and efficiently buffers records in front of your slowest stages and handles latency shocks (such as transient sluggishness from a remote database or API).

==== Executor Details (?)

Not sure if I need anything here.

==== The Spout Pending Register

Say how a tuple is cleared from the pending register when its tree is finally acked and what this means for `maxspoutpending`.

=== Acking and Reliability

Storm's elegant acking mechanism is probably its most significant breakthrough.  It ensures that a tuple and its descendents are processed successfully or fail loudly and it does so with a minimum amount of bookkeeping chatter. The rest of the sections in this chapter, while advanced, ultimately are helpful for architecting and productionizing a dataflow.  This section, however, is comparatively optional -- the whole point of the reliability mechanism is that it Just Works.  It's so fascinating we can't help but include it but if you're not looking to have your brain bent today, feel free to skip it.  Footnote:[This story is complicated enough that I'm going to make two minor simplifications that are areally only interesting if you're knee-deep in the code. We'll let you know about them in the footnotes.]

Here's how it works.

As each tuple destined for an Executor is created, it is given a unique enough ID; in practice, these are 64-bit integers (this will be important later) but I'm going to pretend that, by cosmic luck, each of those integers ends up resembling the name of a Biblical figure.

When a spout produces a tuple -- let's take, for example, one named "Methuselah" -- it notifies the acker to do two things:  to start tracking Methuselah's tuple tree and to inscribe Methuselah's name in that tupletree's Scroll of Ages.  [FOOTNOTE:  Actually, since a tuple can be sent to multiple downstream Executors, it's more appropriate to say it inscribes each of Methuselah's clones in the Scroll of Ages.]

As described above, that tuple will eventually be processed by the downstream Executor's `execute` method, which typically emits tuples and must call `ack` or `fail`, (TODO:  insert details of what happens when a tuple fails).  In the typical case, the Executor's bolt happily calls `emit` 0, 1 or many times and then calls `ack`.  As each emitted tuple is placed in the send queue, the Executor notes its name [FOOTNOTE:  Actually, the names of all its clones.] for later delivery to the acker.  When the bolt calls `ack`, the Executor notifies the acker with the name of the parent and each child.

So if a bolt, receiving a tuple called "Noah," emitted tuples called "Ham" and "Shem," it strikes Noah from the Scroll of Ages but lists Ham and Shem therein.  (TODO:  Rearrange?)  When a bolt emits one or more tuples, the parent is removed but the children are added and so the Scroll of Ages continues to have at least those entries in it. If a bolt received a tuple called "Onan," and emitted nothing, then it would only notify the acker to clear Onan, adding nothing.  Ultimately, for a tupletree to be successfully completed, every descendent must ultimately encounter a bolt that emits nothing.

Up until now, I've made it sound as if each name in the Scroll of Ages is maintained separately.  The actual implementation is far more elegant than that and relies on a few special properties of the XOR function.

First, you can freely rearrange the order in which several terms are XOR'd together:  `Noah XOR Shem XOR Ham` is the same as `Shem XOR Noah XOR Ham` and so forth.  Second, the XOR of a term with itself is 0:  Noah XOR Noah is 0 for anybody.  Do you see where this is going? In our example, (TODO:  Repair so it's Noah's tree)when the Scroll of Ages was first prepared, inscribed on it was only Noah's name.  When the Executor handling that tuple notified back, it didn't have to send Noah, Ham and Shem distinctly; it just sent the single 64-bit integer `Noah XOR Ham XOR Shem`.  So the Scroll of Ages is pretty brief, as Scrolls go; it actually only holds the one entry that is the combined XOR of every tuple ID that has been sent.  So when the acker receives the ack for Noah, namely `Noah XOR Ham XOR Shem`, it XOR`s that single 64-bit entry with the existing tupletree `checksum` storing that `checksum` back to the Scroll of Ages.  (NOTE:  TODO Rework Scroll of Ages metaphor to hold all tupletrees.)

The value at this point is effectively `Noah XOR Shem XOR Ham`. From the first property, the Noah terms cancel out and so our tupletree state is now just `Shem XOR Ham`.

Thanks to the second property, even as acks come in asynchronously, the Scroll of Ages remains correct.  `(Shem XOR Ham) XOR (Shem XOR Abraham) XOR (Ham) XOR (Abraham)` rearranges to provide two Shems, two Hams and two Abrahams.  ,Since, in this example, the family line of Shem and Abraham produced no resulting tuples, we are left with 0.

As soon as that last ack comes in, producing a 0 in the Scroll of Ages, the acker notifies the spout that the tupletree has concluded.  This lets the spout remove that very first tuple from its pending list.  The loop that calls `nexttuple` will, on its next trip through, see the new pending count and, if conditions are right, call `nexttuple`.

This system is thus able to accommodate many millions of active tuples with remarkably little network chatter or memory footprint.  Only the spout's pending tuples are retained for anything except immediate processing.  Now, this comes at a cost, if any downstream tuple fails, the whole tree is retried but since failure is the uncommon case, (and finite RAM is the universal case), this is the right tradeoff.  Second, the XOR trick means a single 64-bit integer is sufficient to track the legacy of an entire tupletree, no matter how large, and a single 64-bit integer is all that has to be tracked and sent to the acker, no matter how many downstream tuples an Executor produces.

If you're scoring at home, for each tupletree, the entire bookkeeping system consumes Order(1) number of tuples, Order(1) size of `checksum` and only as many acks as tuples.

One last note.  You can do the math on this yourself, but 64 bits is enough that the composed XOR of even millions of arbitrary 64-bit integer will effectively never come out to be 0 unless each term is repeated.

=== Lifecycle of a Trident batch

What should you take away from this chapter:

* Understand the lifecycle of partitions within a Trident batch and thus, the context behind partition operations such as Apply or PartitionPersist.
* Understand how the master batch coordinator and spout coordinator for the Kafka spout in particular work together to uniquely and efficiently process all records in a Kafka topic.
* One specific:  how Kafka partitions relate to Trident partitions.
* Understand Trident's transactional mechanism, in the case of a PartitionPersist.
* Understand how Aggregators, Statemap and the Persistence methods combine to give you _exactly once_  processing with transactional guarantees.  Specifically, what an OpaqueValue record will look like in the database and why.

At this point, you've seen how the Storm layer efficiently and reliably handles individual tuples by the millions and billions.  One of the most important things to keep in mind about the Trident layer is that every Trident flow is a Storm flow.  It uses, without modification or improvement, all of Storm's machinery but extends it to provide a simpler interface and the features (like aggregations and  _exactly once_ processing) that make Stream analytics possible.

You might think that we begin by talking about a Trident spout; after all, as you've been using Trident, that's where your flow conceptually begins.  It's time we revealed a confusing clarification:  The Trident spout is actually a Storm bolt.  Viewed from the programmer interface, Trident spouts independently source tuples for downstream processing.

All of the Trident operations you are familiar with -- spouts, each'es, aggregations -- actually take place in Storm bolts.  Trident turns your topology into a dataflow graph that it uses to assign operations to bolts and then to assign those bolts to workers.  It's smart enough to optimize that assignment: It combines operations into bolts so that, as much as possible, tuples are handed off with simple method cause and it arranges bolts among workers so that, as much as possible, tuples are handed off to local Executors.  (connecting material here)

The actual spout of a Trident topology is called the Master Batch Coordinator (MBC).  From Storm's end, it's the dullest possible spout; all it does is emit a tuple describing itself as batch 1 and then a tuple describing itself as batch 2 and so forth, ad infinitum.  (Of course, deciding when to emit those batches, retry them, etc., is quite exciting but Storm doesn't know anything about all that).  Those batch tuples go to the topology's Spout Coordinator.  The Spout Coordinator understands the location and arrangement of records in the external source and ensures that each source record belongs uniquely to a successful Trident batch.

The diagram on the right shows this in action for the GitHub topology.  In this section, we are going to track three Trident matches (labeled 1, 2 and 3) through two parallel Kafka spouts, each pulling from a single Kafka partition.  The Spout Coordinator passes the single seed tuple from the MBC onto each of its spouts, equipping each of them with a starting Kafka offset to read from.  Each spout then requests, from the Kafka broker, a range of messages, beginning at its determined offset and extending to, at most, `Max_Fetch_Bytes` .  If `Max_Fetch_Bytes` were, say, 1000 bytes, and your records were uniformly 300 bytes, Kafka would return to the spout just the next three records, totalling 900 bytes.  You must set `Max_Fetch_Bytes` larger than your largest expected record; the Kafka spout will fail if a record is too large to fit in a single batch.

In most cases, records have a fairly bounded spread of sizes around a typical value.  The GitHub records, for example, are (TODO:  Find size x+- y bytes long).  This means that a `Max_Fetch_Bytes` size of (TODO:  value) might return as few as (A) and as many as (B).  Pesky but harmless.  If the size variance of your records is large enough to make this a real problem, unfortunately, you’ll have to modify the Kafka spout.

Let’s pause there and I’m going to tell you a story:  Here’s the system chimpanzee school children follow when they go on a field trip.  At the start of the day, a set of school buses pull up in waves at the school.  The first graders all file onto the first set of buses and head off first followed by the set of buses for the second graders and so forth.  As each bus pulls up at the museum, all the kids come off that bus in a group, known as a partition.  All get off in a group (which we’ll call a partition) and each group is met by a simple-minded docent assigned to that group by the museum.  Now, chimpanzees are an unruly sort, but they are able to be well-behaved in at least the following way:  All the chimpanzees in a partition follow the same path through the museum and no chimpanzee in the partition ever cuts ahead of another chimp.  So, the third kid off the bus will see the same paintings as the second kid and the fourth kid and she’ll see each of those paintings some time after the second kid did and some time before the fourth kid did.  Each docent memorizes the number of students in its assigned partition, patiently takes a place in line after the last chimpanzee and follows along with them through the museum. If you visited the museum on chimpanzee field trip day, well, it can sometimes be just as chaotic as you’d expect, what with kids looking at the pointless paintings from up close and afar, a set of them hooting in recognition at the exhibition on ostrolopithazines and others gather to intently study paintings they’ll be discussing later in class.  If you were to try to manage the ebb and flow of each of these partition groups in bulk, you wouldn’t decrease the chaos; you’d just make it so nobody ever got through the hallways at all.  No, the only good system is the one that lets each chimpanzee browse at his or her own pace.

Most exhibits are enjoyed by EACH chimpanzee individually, and so the chimpanzees file by as they come.  If a set of third graders and a set of first graders arrive at an exhibit at the same time, they’d file through the exhibit in whatever in or leave order happened by circumstance; that’s ok, because, of course, within those partitions, the good little chimpanzee boys and girls were obeying the field trip rules; no first grader ever jumped ahead of the first grader it followed
and no third grader ever jumped ahead of the third grader it followed.

Now, at some points during the field trip, the chimpanzees are to discuss an exhibit as a partition.  When the first chimpanzee in that partition arrives at an exhibit, the exhibit’s Operator will ask her to stand to the side and direct each chimpanzee in the partition to gather behind her.  When, at some point, the docent shows up (last in line because of the field trip rules), the Operator checks that everyone is there by counting the number of kids in the partition and checking against the count that the docent carries.  With that ritual satisfied, the Operator conducts the `partitionQuery` Q&A session.  Each student, thus enlightened as a group, is then sent along to the next exhibit in exactly the original partition order.

As you can see, the students are able to enjoy the exhibits in the museum singly or in groups without any more coordination than is necessary.  However, at the end of the day, when it’’s time to go home, a much higher level of commitment to safety is necessary.  What happens when it’s time to return is this:  As each partition group files out of the museum, it gathers back at its original school bus.  Just as in the group discussion, the bus Operator notices when the docent shows up (signaling the end of the partition) and compares the actual to expected count.  Once satisfied that the full partition is present, it signals the Master Batch Coordinator for that grade that all the bus’s students are present.  Once the Master Batch Coordinator has received the "ready" signal from all the buses in a grade, it signals all the bus Operators that they are approved to transport the students back to school.  Finally, once safely back at school, each bus Operator radios the Master Batch Coordinator of their safe arrival, allowing the MBC to declare the field trip a success.

=== _exactly once_ Processing

Storm ensures that every tuple within a Storm or Trident dataflow is handled reliably and correctly.  The difficulty comes when your dataflow must interact with an external resource, like a database or perform a task _exactly once_ like when performing a count or other aggregation.  This harder guarantee is provided by Trident’s transactional functionality.  If you’re coming from a traditional database world, don’t reach too much into the word "transaction."  First, what we’re discussing here takes place entirely outside of and entirely separate from any native transactional guarantees by the database.  Second, it only provides a form of eventual consistency, as you’re about to see.

Trident’s `partitionPersist` and `persistentAggregate` classes provide their operations the following guarantee when applying an operation to a partition within a batch.  First, all batches before that one will have been processed successfully.  Second, no batch after that one will have ever been attempted.  In return, the Aggregator must promise that, given the same records and prior value, it will return an acceptably-identical result.  (FOOTNOTE:  Note, I said "acceptably-identical," not identical.  For instance, if your dataflow annotated business news articles with the share prices of companies mentioned within, those prices may have changed from one attempt of a batch to the next.  In this case, the "acceptably-identical" result would have mildly-diverging values.)  Here’s how that guarantee is used by a `persistentAggregate` storing counts into an external datastore.

The `persistentAggregate` functionality in Trident has multiple moving parts, all of them modular, allowing the same classes to be used in simpler form for operations like `partitionPersist`.  Since the important thing is that you understand how a `persistentAggregate` works, not how to rewrite one, we’re going to describe how its functional parts finally work together, not build it up piece by piece.

As individual aggregable records roll in, each is handed to the Aggregator for its group within the partition; if it’s the first member of a group, the Aggregator is created and prepared.  In the case of a Combiner Aggregator or a Reducer Aggregator, only the running value needs to be tracked; an Accumulating Aggregator may be buffering those values internally.

When the partition is complete and the `persistentAggregate` receives the commit signal from the MBC, it therefore has on hand the following:  All group keys seen in this partition and for each of those keys, an Aggregator instance, fat from having consumed all the records in that group.

It now needs to retrieve the prior existing value, if any, from the backing store.  For example, in the case of the simple counting aggregation, it needs only the primitive integer holding the previous batch’s value.  In the case where we accumulated a complex profile, it’s a `HashMap` describing that profile.

(TODO: Perhaps more here …)

The `persistentAggregate` wrapper hands the cache map (the first backing layer) the full set of group keys in the given partition, requesting their value.  Assuming things are behaving well, the cache map will have those values, hot and ready, in memory --- but of course, it may be missing some or all.  The cache map, using exactly the same interface, asks the concrete backing store’s `StateMap` for the values it lacks.  (TODO:  What does it send back when a value is missing?)  The cache map accepts the results and proudly presents the full set of values back to the `persistentAggregate` wrapper.  The wrapper then promptly finalizes the aggregated values.  It then hands a map of group keys and their updated values back to the backing store.  The cache map player stores all those values in its cache, possibly triggering the least-recently-used values to be discarded.  The cache map then, in turn, hands the full set of values to the concrete datastore, which persists them to the external database.  Note that only a fraction of values for any given partition are typically read from the database but that every value in a partition is written back.

Now, here’s the clever part.  The concrete datastore accepts what it’s given, the actual value to store, and it returns what it’s asked for, the value as of the last batch.  But it stores within the record for a given group key the following things:  the transaction ID of the current batch, the newly-updated value and the prior value that it was based on; let’s call the values the "aligned" value and the "pre-aligned" value, respectively.  At whatever later time it’s asked to retrieve the value for that record, it demands to know the current transaction ID as well.

Now, let’s think back to the transactional guarantee we described above.  Suppose the record it retrieves has a transaction ID of 8 and the aligned transaction ID is 12.  Great!  The backing store knows that, although this record wasn’t involved, batches 8, 9, 10 and 11 were all processed successfully.  It then takes the aligned value from batch 8 and faithfully report it as the value to update.

(TODO:  SIDEBAR: It could happen that the record it retrieves shows a transaction ID of 12.  It might be that this worker is retrying an earlier failed attempt, it might be that this worker fell off the grid and it’s seeing the result of the retry due to its laziness.)

It might be, as described in the sidebar, that the transaction ID is 12.  Remember, the request is for the value prior to the current batch; luckily, that’s exactly what was stored in the pre-aligned value and so that’s what is returned.  Now, you see why it’s important that the Aggregator promises acceptably-identical results, given the same records and prior value; you’re not allowed to care which attempt of a retry is the last one to complete.

This is all done behind the scenes and you never have to worry about it.  In fact, the class that hides this transactional behavior is called the `opaquevalue` class and this type of dataflow is what Trident calls an `OpaqueTransactional` topology.

For folks coming from a traditional database background, please notice that while we use the word "transactional" here, don’t read too much into that.  First, we’re not using and not relying on any native transactional guarantee in the commit to the external database.  The transactional behavior we’ve described covers the entire progress of a batch, not just the commit of any given partition to the database.  Second, the transactional behavior is only eventually consistent.  In practice, since the Master Batch Coordinator signals all `persistentAggregate` to commit simultaneously, there is very little jitter among attempts to commit.  If your database administrator is doing her job, in normal circumstances, an external read will not notice misaligned batches.

Of course, all this machinery was put in place to tolerate the fact that sometimes, a subset of workers might hang or die trying to commit their partition within a batch.  In that case, a read of the database would return some values current as of, say, batch 12, while (until the retry happens) the failed workers’ records are only up to date as of batch 11.


=== Walk-through of the Github dataflow

Let's walk through the batch lifecycle using the Github dataflow from the Intro to Storm chapter (TODO:  ref).

(NOTE:  TODO:  In chapter on Trident tuning, make clear that we are not talking about Storm tuning and some of our advice, especially around `maxspoutpending` will be completely inappropriately for a pure Storm flow.)

(TODO: Missing Section. This is covered somewhat above, but we need to either specifically do a walkthrough of Github, or wind it into what comes)
