== Rules of Scaling ==


=== The rules of scaling ===

* Storage (Disk)                             is free, and infinite in size 
* Processing (CPU)                           is free, except when it isn't
* Flash Storage (SSD)                        is pricey, and cruelly limited in size 
* Memory (RAM)                               is expensive, and cruelly limited in size 

* Memory                                     is infinitely fast.
* Streaming data across the network          is ever-so-slightly faster than streaming from disk
* Streaming data from disk                   is the limiting factor if you're doing things right...
* ... unless processing (CPU)                is the limiting factor in speed
* Reading data in pieces from SSD            is adequate in some cases, assuming you fit
* Reading data in pieces from disk           is infinitely slow
* Reading data in pieces across the network  is even slower

Most importantly,

* Humans are important, robots are cheap.
 
=== Optimize first, and typically only, for Joy ===

The most important rule for writing scalable code is

    Optimize first, and typically only, for Joy

There's no robust measure for programmer productivity. 'Lines of code' certainly isn't -- this book mostly exists to help you write as few lines of code as possible.

But the fundamental personality traits that drive people to become programmers and data scientists are a love of a) discovering answers, b) discovering simplicities, c) discovering efficiencies.

Phase 1: discover answer

Do this quickly 

For anything that stops our solution doesn't have to be elegant as long as it is readable.

If this is a process that will endure, your next step is to simplify it

Do not even think about what is happening inside the mapper and reducer until you have arrived at the simplest transformation flow you can.

Optimizing your code path might subtract 30% of your run time at the cost of simplicity.
Optimizing your data path can in many cases subtract 90% of your run time

Find the parts that are annoying -- 

Corrolaries:

    Automate out of boredom or terror, never efficiency


==== Steps ====

    (read and label the data)
    (send the data across the network)
    (sort each batch)
    (process the data in each batch and output it)


=== How to size your cluster ===

* If you are IO-bound, use the cheapest machines available, as many of them as you care to get
* If you are CPU-bound, use the fastest machines available, as many of them as you care to get
* If the shuffle is the slowest step, use a cluster that is 50% to 100% as large as the mid-flight data.

For each of 

* Local Disk
* EBS
* SSD
* S3
* MySQL (local)
* MySQL (network)
* HBase (network)
* in-memory
* Redis (local)
*  Redis (network)

Compare throughput of:

* random readss    
* streaming reads  
* random writes 
* streaming writes

	==== Transfer ====

	cp                | A.1     => A.1
	cp                | A.1     => A.2
	scp               | A.1     => A.2
	scp               | A.1     => B.1
	hdp-put           | A.1     => hdfs
	hdp-put           | all.1   => hdfs

	hdp-cp            | hdfs-X  => hdfs-X
	distcp            | hdfs-X  => hdfs-Y

	db read           | hbase-T => hdfs-X
	db read/write     | hbase-T => hbase-U

	db write          | hdfs-X  => hbase-T

	==== Map-only ====

	null              | s3      => hdfs
	null              | hdfs    => s3
	null              | s3      => s3

	identity (stream) | s3      => hdfs
	identity (stream) | hdfs    => s3
	identity (stream) | s3      => s3

	reverse           | s3      => hdfs
	reverse           | hdfs    => s3
	reverse           | s3      => s3

	pig_latin         | s3      => hdfs
	pig_latin         | hdfs    => s3
	pig_latin         | s3      => s3

	=== Reduce ===

	partitioned sort  | hdfs    => hdfs
	partitioned sort  | s3      => hdfs
	partitioned sort  | hdfs    => s3
	partitioned sort  | s3      => s3

	total sort        | hdfs    => hdfs

=== Big Midflight Output ===


=== Many Midflight Records ===

adjacency list 

=== Big Reduce Output ===

cross | hdfs => hdfs

=== High CPU ===

bcrypt line       | hdfs => hdfs



