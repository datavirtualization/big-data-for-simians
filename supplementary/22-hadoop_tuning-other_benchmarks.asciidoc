=== How big is a Terabyte? ===

colobus-master-0 /usr/lib/hadoop$ hdp-du s3n://bigdata.chimpy.us/data/results/wikipedia/full
Found 5 items
s3n://bigdata.chimpy.us/data/results/wikipedia/full/page_metadata      	     2233601837	         2.1 GB
s3n://bigdata.chimpy.us/data/results/wikipedia/full/pagelinks          	    49777662270	        46.4 GB
s3n://bigdata.chimpy.us/data/results/wikipedia/full/pageviews          	   846600801080	       788.5 GB
s3n://bigdata.chimpy.us/data/results/wikipedia/full/redirects_page_metadata	      400190601	       381.7 MB
s3n://bigdata.chimpy.us/data/results/wikipedia/full/undirected_pagelinks	    13182759459	        12.3 GB
                                                       5 entries       	   912195015247	       849.5 GB

wikipedia corpus: 	40 GB
Hour-by-hour weather: 	12 GB

Every pageview of the World Cup website during the year: 	1TB
Every pitch of every MLB baseball game, with full game state, 2007-2011 < 100 GB

10 trillion digits of Pi
Google Books N-Grams			2 TB
Common Crawl Web Corpus			60 TB
Planet.osm contains the entire planet. This is a snapshot of the current data, usually from last Wednesday. This is almost 20 GB compressed or 150 GB uncompressed XML.


For the below: the *must* haves, not the "gee that would be swell"

How much data under management do customers have, month on month, bringing in?
* cold load?
* hot load?
How many disparate data sources?

How large is analytic team?

* "I use Amazon cloud infrastructure; it's important you run on AWS"
* "I don't care where
* Exploratory data analysis


From http://developer.yahoo.com/blogs/hadoop/posts/2010/05/scalability_of_the_hadoop_dist/ -- May 5, 2010

	Throughput
Get block locations	126,119 ops/s
Create new block	5,600 ops/s

Throughput
Average read throughput	66 MB/s
Average write throughput	40 MB/s

terasort

 On a 100-node cluster with a quad dual-core CPU hardware the running time should be roughly within
 10 minutes (one of our customers sorted 1TB in 6 minutes on a 76-node cluster, the numbers are
 likely to go down with new 12-core CPU machines). 

March 30, 2010
 
 4 1TB hard disks in a JBOD (Just a Bunch Of Disks) configuration
2 quad core CPUs, running at least 2-2.5GHz
16-24GBs of RAM (24-32GBs if you’re considering HBase)
Gigabit Ethernet

http://www.cs.duke.edu/starfish/mr-apps.html

https://issues.apache.org/jira/secure/attachment/12524797/pig-0.8.1-vs-0.9.0.png

	100GB, 4 EC2 m1.large nodes -- most in ~ 300 seconds == 25 GB/node for 5 min == 5 GB/min

https://issues.apache.org/jira/browse/PIG-2397


From Starfish:

	"MapReduce programs.
	The default experimental setup used in this paper is a single-rack Hadoop cluster running on 16 Amazon EC2 nodes of the c1.medium type. Each node runs at most 3 map tasks and 2 reduce tasks concurrently. WordCount processes 30GB of data generated using the RandomTextWriter program in Hadoop. TeraSort processes 50GB of data generated using Hadoop’s TeraGen program"

	io.sort.spill.percent 	   0.80	   0.80	   0.80	   0.80
	io.sort.record.percent	   0.50	   0.05	   0.15	   0.15
	io.sort.mb            	 200.00	  50.00	 200.00	 200.00
	io.sort.factor        	  10.00	  10.00	  10.00	 100.00
	mapred.reduce.tasks   	  27.00	   2.00	  27.00	 400.00
	Running Time (sec)    	 785.00	 407.00	 891.00	  60.00


From http://www.cs.duke.edu/starfish/files/socc10-mr-opt.pdf


	Parameter Name                  	Brief Description and Use                                        	Default Value 	Values Considered
	mapred.reduce.tasks                    	Number of reducer tasks                                             	1       	[5,300]
	io.sort.mb                             	Size in MegaBytes of map-side buffer for sorting key/value pairs    	100     	[100,200]
	io.sort.record.percent                 	Fraction of io.sort.mb dedicated to metadata storage                	0.05    	[0.05,0.15]
	io.sort.factor                         	Number of sorted streams to merge at once during sorting            	10      	[10,500]
	io.file.buffer.size                    	Buffer size used to read/write (intermediate) sequence files        	4K      	32K
	mapred.child.java.opts                 	Java control options for all mapper and reducer tasks               	-Xmx200m	-Xmx[200m,300m]
	mapred.job.shuffle.input.buffer.percent	% of reducer task's heap used to buffer map outputs during shuffle  	0.7     	0.7,0.8
	mapred.job.shuffle.merge.percent       	Usage threshold of mapred.job.shuffle.input.buffer.percent to trigger	0.66    	0.66,0.8
						reduce-side merge in parallel with the copying of map outputs
	mapred.inmem.merge.threshold           	Another reduce-side trigger for in-memory merging; off when 0       	1000    	0
	mapred.job.reduce.input.buffer.percent 	% of reducer task's heap to buffer map outputs while applying reduce	0       	0,0.8
	dfs.replication                        	Block replication factor in Hadoop's HDFS filesystem                	3       	2
	dfs.block.size                         	HDFS block size (data size processed per mapper task in our setting)	64MB    	128MB



=== JVM Tuning ===

Abandon Hope All Ye who Enter Here
 From  http://www.infoq.com/interviews/szegedi-performance-tuning[InfoQ: Attila Szegedi on JVM and GC Performance Tuning at Twitter]

__________________________________________________________________________
So when go and deal with a performance problem with some team within Twitter, are you looking at the code first or do you tend to look at the way the garbage collector’s configured or where do you start?
Well, a garbage collector is a global service for a particular JVM and as such, its own operation is affected by the operation of all the code in the JVM which is the Java libraries, third party libraries that have been used and so on, which means that, you can’t really, or, let me put it this way: if you need to look at the application code in order to tune the garbage collector, then you are doing it wrong because from the point of view of the application, garbage collectors are a blackbox and vice-versa.

From the point of view of the garbage collector, the application is a blackbox. You only just see the statistical behavior basically: allocation rates, the typical duration of life of the objects and so on. So, the correct way to tune the GC is to actually inspect the GC logs, see the overall utilization of memory, memory patterns, GC frequencies - observe it over time and tune with that in mind.
And you would do that level of logging in production?
Yes, we do. It’s not that heavy because GC will only log when it does something. Now, if it’s doing something too frequently, then your problem is not the logging; then your problem is that it’s doing something too frequently and when it’s sufficiently nicely tuned, then it’s infrequent than compared to the work that it has to do to clean up memory, just the cost of writing a line to the log is completely negligible. You don’t really perceive that.
So when we are talking about tuning the collector, we are mostly talking about the length and frequency of pauses, right?
Yes, that’s the thing that bites us, yes.
What are the main factors that contribute to that within HotSpot. Do you use HotSpot? So within the HotSpot collector?
Yes. So, within HotSpot, the frequency and duration of the garbage collector pauses; well, generally: if you had a JVM with infinite memory, then you will never have to GC, right? And if you have a JVM with a single byte of free memory then you are GC-ing all the time. And between the two extremes, you have an asymptotically decreasing proportion of your CPU going towards GC which basically means that the best way to minimize the frequencies of your GC is to give your JVM as much memory as you can. Specifically, the frequency of minor GCs is pretty much exactly inversely proportional to the size of the new generation. And as for the old generation GCs, but you really want to avoid those altogether. So, you want to tune your systems so that those never happen. It’s another question whether it’s actually possible to achieve in a non-trivial system with a HotSpot, it’s hard.
__________________________________________________________________________


Using http://www.textuality.com/bonnie/advice.html[bonnie] for disk benchmarking:

--------------------
$ bonnie -d /tmp -s 2000 -m worblehat-Hitachi-HTS725050A9A362 / File '/tmp/Bonnie.97743', size: 2097152000

                 -------Sequential Output-------- ---Sequential Input-- --Random--
                 -Per Char- --Block--- -Rewrite-- -Per Char- --Block--- --Seeks---
Machine      GB   M/sec %CPU M/sec %CPU M/sec %CPU M/sec %CPU M/sec %CPU  /sec %CPU
worblehat     2    58.4 54.2  92.2 13.3  57.9  6.2 157.9 100   5593  100  2067  2.6   
worblehat     8    73.6 67.6  70.6 10.2  31.6  4.2  68.9 59.6  53.8  3.8   185  1.8
worblehat    32    48.7 55.0  53.4 18.6  22.6  2.7  54.4 47.7  55.8  4.8    76  1.2


In phase 1, Bonnie wrote the file by doing 2000 million putc() macro invocations
In phase 2, Bonnie wrote the 2000-Mb file using efficient block writes
In phase 3, Bonnie ran through the 2000-Mb file just created, changing each block and rewriting it
In phase 4, Bonnie read the file using 2000 million gets() macro invocations
In phase 5, Bonnie read the 2000-Mb file using efficient block reads
In phase 6, Bonnie created 4 child processes, and had them execute 4000 seeks to random locations in the file. On 10% of these seeks, they changed the block that they had read and re-wrote it.

worblehat ||                      | 
2000      || MB                   | 
  58.4    || r  seq char   M/sec  | An output rate of 58.4 M per second.
  54.2    || r  seq char   %CPU   | ... consuming 54.2% of one CPU's time.
  92.2    || r  seq block  M/sec  | Output rate of 92.2 M per second.
  12.3    || r  seq block  %CPU   | ... consuming 12.3% of one CPU's time.
  57.9    || rw seq rewrt  M/sec  | Cover 57.9 M per second.
   6.2    || rw seq rewrt  %CPU   | ... consuming 6.2% of one CPU's time.
 157.9    || w  seq char   M/sec  | Input rate of 157.9 M per second.
 100      || w  seq char   %CPU   | This work consumed 100% of one CPU's time. This is amazingly high. The 2GB file is probably too small to be an effective test.
5593      || w  seq block  M/sec  | Input rate of 5592 M per second.
 100      || w  seq block  %CPU   | ... this work consumed 100% of one CPU's time.
2067      || r  rand seeks /sec   | Effective seek rate was 2067 seeks per second.
   2.6    || r  rand seeks %CPU   | ... consuming 2.6% of one CPU's time.

--------------------

	
=== PigMix ===
	
quoted from PigMix docs:


....

Run date: Jun 11, 2011, run against top of trunk as of that day.
All of these runs have been done on a cluster with 26 slaves plus one machine acting as the name node and job tracker

Test        	Pig run time	Java run time	Multiplier
PigMix_1    	130         	139          	0.94
PigMix_2    	66          	48.67        	1.36
PigMix_3    	138         	107.33       	1.29
PigMix_4    	106         	78.33        	1.35
PigMix_5    	135.67      	114          	1.19
PigMix_6    	103.67      	74.33        	1.39
PigMix_7    	77.67       	77.33        	1.00
PigMix_8    	56.33       	57           	0.99
PigMix_9    	384.67      	280.33       	1.37
PigMix_10   	380         	354.67       	1.07
PigMix_11   	164         	141          	1.16
PigMix_12   	109.67      	187.33       	0.59
PigMix_13   	78          	44.33        	1.76
PigMix_14   	105.33      	111.67       	0.94
PigMix_15   	89.67       	87           	1.03
PigMix_16   	87.67       	75.33        	1.16
PigMix_17   	171.33      	152.33       	1.12
Total       	2383.67     	2130         	1.12
Weighted Avg	            	             	1.16


Features Tested
Based on a sample of user queries, PigMix includes tests for the following features.

Data with many fields, but only a few are used.
Reading data from maps.
Use of bincond and arithmetic operators.
Exploding nested data.
Load bzip2 data
Load uncompressed data
join with one table small enough to fit into a fragment and replicate algorithm.
join where tables are sorted and partitioned on the same key
Do a cogroup that is not immediately followed by a flatten (that is, use cogroup for something other than a straight forward join).
group by with only algebraic udfs that has nested plan (distinct aggs basically).
foreachs with nested plans including filter and implicit splits.
group by where the key accounts for a large portion of the record.
group all
union plus distinct
order by
multi-store query (that is, a query where data is scanned once, then split and grouped different ways).
outer join
merge join
multiple distinct aggregates
accumulative mode
The data is generated so that it has a zipf type distribution for the group by and join keys, as this models most human generated
data.
Some other fields are generated using a uniform data distribution.

Scalability tests test the following:

Join of very large data sets.
Grouping of very large data set.
Query with a very wide (500+ fields) row.
Loading many data sets together in one load


Proposed Data
Initially, four data sets have been created. The first, "page_views", is 10 million rows in size, with a schema of:

Name             	Type       	Average Length	Cardinality	Distribution	Percent Null
user             	string     	20            	1.6M       	zipf        	           7
action           	int        	X             	2          	uniform     	           0
timespent        	int        	X             	20         	zipf        	           0
query_term       	string     	10            	1.8M       	zipf        	          20
ip_addr          	long       	X             	1M         	zipf        	           0
timestamp        	long       	X             	86400      	uniform     	           0
estimated_revenue	double     	X             	100k       	zipf        	           5
page_info        	map        	15            	X          	zipf        	           0
page_links       	bag of maps	50            	X          	zipf        	          20

The second, "users", was created by taking the unique user keys from "page_views" and adding additional columns.

Name   	Type  	Average Length	Cardinality	Distribution	Percent Null
name   	string	20            	1.6M       	unique      	           7
phone  	string	10            	1.6M       	zipf        	          20
address	string	20            	1.6M       	zipf        	          20
city   	string	10            	1.6M       	zipf        	          20
state  	string	2             	1.6M       	zipf        	          20
zip    	int   	X             	1.6M       	zipf        	          20

The third, "power_users", has 500 rows, and has the same schema as users. It was generated by skimming 500 unique names from
users. This will produce a table that can be used to test fragment replicate type joins.

The fourth, "widerow", has a very wide row (500 fields), consisting of one string and 499 integers.

"users", "power_users", and "widerow" are written in ASCII format, using Ctrl-A as the field delimiter. They can be read using
PigStorage.

"page_views" is written in as text data, with Ctrl-A as the field delimiter. Maps in the file are delimited by Ctrl-C
between key value pairs and Ctrl-D between keys and values. Bags in the file are delimited by Ctrl-B between tuples in the bag.
A special loader, PigPerformance loader has been written to read this format.

PigMix2 include 4 more data set, which can be derived from the original dataset:

A = load 'page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp, estimated_revenue, page_info, page_links);
B = order A by user $parallelfactor;
store B into 'page_views_sorted' using PigStorage('\u0001');

alpha = load 'users' using PigStorage('\u0001') as (name, phone, address, city, state, zip);
a1 = order alpha by name $parallelfactor;
store a1 into 'users_sorted' using PigStorage('\u0001');

a = load 'power_users' using PigStorage('\u0001') as (name, phone, address, city, state, zip);
b = sample a 0.5;
store b into 'power_users_samples' using PigStorage('\u0001');

A = load 'page_views' as (user, action, timespent, query_term, ip_addr, timestamp,
        estimated_revenue, page_info, page_links);
B = foreach A generate user, action, timespent, query_term, ip_addr, timestamp, estimated_revenue, page_info, page_links,
user as user1, action as action1, timespent as timespent1, query_term as query_term1, ip_addr as ip_addr1, timestamp as timestamp1, estimated_revenue as estimated_revenue1, page_info as page_info1, page_links as page_links1,
user as user2, action as action2, timespent as timespent2, query_term as query_term2, ip_addr as ip_addr2, timestamp as timestamp2, estimated_revenue as estimated_revenue2, page_info as page_info2, page_links as page_links2;
store B into 'widegroupbydata';


Proposed Scripts
Scalability
Script S1

This script tests grouping, projecting, udf envocation, and filtering with a very wide row. Covers scalability feature 3.

A = load '$widerow' using PigStorage('\u0001') as (name: chararray, c0: int, c1: int, ..., c500: int);
B = group A by name parallel $parrallelfactor;
C = foreach B generate group, SUM(A.c0) as c0, SUM(A.c1) as c1, ... SUM(A.c500) as c500;
D = filter C by c0 > 100 and c1 > 100 and c2 > 100 ... and c500 > 100;
store D into '$out';
Script S2
This script tests joining two inputs where a given value of the join key appears many times in both inputs. This will test pig's
ability to handle large joins. It covers scalability features 1 and 2.

TBD

Features not yet tested: 4.

Latency
Script L1

This script tests reading from a map, flattening a bag of maps, and use of bincond (features 2, 3, and 4).

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp,
        estimated_revenue, page_info, page_links);
B = foreach A generate user, (int)action as action, (map[])page_info as page_info,
    flatten((bag{tuple(map[])})page_links) as page_links;
C = foreach B generate user,
    (action == 1 ? page_info#'a' : page_links#'b') as header;
D = group C by user parallel 40;
E = foreach D generate group, COUNT(C) as cnt;
store E into 'L1out';
Script L2

This script tests using a join small enough to do in fragment and replicate (feature 7).

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp,
        estimated_revenue, page_info, page_links);
B = foreach A generate user, estimated_revenue;
alpha = load '/user/pig/tests/data/pigmix/power_users' using PigStorage('\u0001') as (name, phone,
        address, city, state, zip);
beta = foreach alpha generate name;
C = join B by user, beta by name using 'replicated' parallel 40;
store C into 'L2out';
Script L3

This script tests a join too large for fragment and replicate. It also contains a join followed by a group by on the same key,
something that pig could potentially optimize by not regrouping.

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp,
        estimated_revenue, page_info, page_links);
B = foreach A generate user, (double)estimated_revenue;
alpha = load '/user/pig/tests/data/pigmix/users' using PigStorage('\u0001') as (name, phone, address,
        city, state, zip);
beta = foreach alpha generate name;
C = join beta by name, B by user parallel 40;
D = group C by $0 parallel 40;
E = foreach D generate group, SUM(C.estimated_revenue);
store E into 'L3out';
Script L4

This script covers foreach generate with a nested distinct (feature 10).

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp,
        estimated_revenue, page_info, page_links);
B = foreach A generate user, action;
C = group B by user parallel 40;
D = foreach C {
    aleph = B.action;
    beth = distinct aleph;
    generate group, COUNT(beth);
}
store D into 'L4out';
Script L5

This script does an anti-join. This is useful because it is a use of cogroup that is not a regular join (feature 9).

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp,
        estimated_revenue, page_info, page_links);
B = foreach A generate user;
alpha = load '/user/pig/tests/data/pigmix/users' using PigStorage('\u0001') as (name, phone, address,
        city, state, zip);
beta = foreach alpha generate name;
C = cogroup beta by name, B by user parallel 40;
D = filter C by COUNT(beta) == 0;
E = foreach D generate group;
store E into 'L5out';
Script L6

This script covers the case where the group by key is a significant percentage of the row (feature 12).

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp,
        estimated_revenue, page_info, page_links);
B = foreach A generate user, action, (int)timespent as timespent, query_term, ip_addr, timestamp;
C = group B by (user, query_term, ip_addr, timestamp) parallel 40;
D = foreach C generate flatten(group), SUM(B.timespent);
store D into 'L6out';
Script L7

This script covers having a nested plan with splits (feature 11).

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader() as (user, action, timespent, query_term,
            ip_addr, timestamp, estimated_revenue, page_info, page_links);
B = foreach A generate user, timestamp;
C = group B by user parallel 40;
D = foreach C {
    morning = filter B by timestamp < 43200;
    afternoon = filter B by timestamp >= 43200;
    generate group, COUNT(morning), COUNT(afternoon);
}
store D into 'L7out';
Script L8

This script covers group all (feature 13).

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp,
        estimated_revenue, page_info, page_links);
B = foreach A generate user, (int)timespent as timespent, (double)estimated_revenue as estimated_revenue;
C = group B all;
D = foreach C generate SUM(B.timespent), AVG(B.estimated_revenue);
store D into 'L8out';
Script L9

This script covers order by of a single value (feature 15).

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp,
        estimated_revenue, page_info, page_links);
B = order A by query_term parallel 40;
store B into 'L9out';
Script L10

This script covers order by of multiple values (feature 15).

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent:int, query_term, ip_addr, timestamp,
        estimated_revenue:double, page_info, page_links);
B = order A by query_term, estimated_revenue desc, timespent parallel 40;
store B into 'L10out';
Script L11

This script covers distinct and union and reading from a wide row but using only one field (features: 1, 14).

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp,
        estimated_revenue, page_info, page_links);
B = foreach A generate user;
C = distinct B parallel 40;
alpha = load '/user/pig/tests/data/pigmix/widerow' using PigStorage('\u0001');
beta = foreach alpha generate $0 as name;
gamma = distinct beta parallel 40;
D = union C, gamma;
E = distinct D parallel 40;
store E into 'L11out';
Script L12

This script covers multi-store queries (feature 16).

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp,
        estimated_revenue, page_info, page_links);
B = foreach A generate user, action, (int)timespent as timespent, query_term,
    (double)estimated_revenue as estimated_revenue;
split B into C if user is not null, alpha if user is null;
split C into D if query_term is not null, aleph if query_term is null;
E = group D by user parallel 40;
F = foreach E generate group, MAX(D.estimated_revenue);
store F into 'highest_value_page_per_user';
beta = group alpha by query_term parallel 40;
gamma = foreach beta generate group, SUM(alpha.timespent);
store gamma into 'total_timespent_per_term';
beth = group aleph by action parallel 40;
gimel = foreach beth generate group, COUNT(aleph);
store gimel into 'queries_per_action';
Script L13 (PigMix2 only)

This script covers outer join (feature 17).

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
	as (user, action, timespent, query_term, ip_addr, timestamp, estimated_revenue, page_info, page_links);
B = foreach A generate user, estimated_revenue;
alpha = load '/user/pig/tests/data/pigmix/power_users_samples' using PigStorage('\u0001') as (name, phone, address, city, state, zip);
beta = foreach alpha generate name, phone;
C = join B by user left outer, beta by name parallel 40;
store C into 'L13out';
Script L14 (PigMix2 only)

This script covers merge join (feature 18).

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/page_views_sorted' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp, estimated_revenue, page_info, page_links);
B = foreach A generate user, estimated_revenue;
alpha = load '/user/pig/tests/data/pigmix/users_sorted' using PigStorage('\u0001') as (name, phone, address, city, state, zip);
beta = foreach alpha generate name;
C = join B by user, beta by name using 'merge';
store C into 'L14out';
Script L15 (PigMix2 only)

This script covers multiple distinct aggregates (feature 19).

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp, estimated_revenue, page_info, page_links);
B = foreach A generate user, action, estimated_revenue, timespent;
C = group B by user parallel 40;
D = foreach C {
    beth = distinct B.action;
    rev = distinct B.estimated_revenue;
    ts = distinct B.timespent;
    generate group, COUNT(beth), SUM(rev), (int)AVG(ts);
}
store D into 'L15out';
Script L16 (PigMix2 only)

This script covers accumulative mode (feature 20).

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp, estimated_revenue, page_info, page_links);
B = foreach A generate user, estimated_revenue;
C = group B by user parallel 40;
D = foreach C {
    E = order B by estimated_revenue;
    F = E.estimated_revenue;
    generate group, SUM(F);
}

store D into 'L16out';
Script L17 (PigMix2 only)

This script covers wide key group (feature 12).

register pigperf.jar;
A = load '/user/pig/tests/data/pigmix/widegroupbydata' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
    as (user, action, timespent, query_term, ip_addr, timestamp,
        estimated_revenue, page_info, page_links, user_1, action_1, timespent_1, query_term_1, ip_addr_1, timestamp_1,
        estimated_revenue_1, page_info_1, page_links_1, user_2, action_2, timespent_2, query_term_2, ip_addr_2, timestamp_2,
        estimated_revenue_2, page_info_2, page_links_2);
B = group A by (user, action, timespent, query_term, ip_addr, timestamp,
        estimated_revenue, user_1, action_1, timespent_1, query_term_1, ip_addr_1, timestamp_1,
        estimated_revenue_1, user_2, action_2, timespent_2, query_term_2, ip_addr_2, timestamp_2,
        estimated_revenue_2) parallel 40;
C = foreach B generate SUM(A.timespent), SUM(A.timespent_1), SUM(A.timespent_2), AVG(A.estimated_revenue), AVG(A.estimated_revenue_1), AVG(A.estimated_revenue_2);
store C into 'L17out';
Features not yet covered: 5 (bzip data)

....
